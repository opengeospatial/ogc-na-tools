{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OGC Naming Authority tools","text":""},{"location":"#purpose","title":"Purpose","text":"<p>This repository contains tools used to maintain controlled vocabularies and knowledge assets managed by the OGC Naming Authority. Such tools may have wider general applicability and be refactored into tool specific repositories.</p>"},{"location":"#scope","title":"Scope","text":"<p>The tools here manage ETL processes for ingesting source data into a dynamic knowledge graph. Whilst this is quite a generic scope, this provides examples of how to use a range of resources that others may reuse to achieve similar results.</p>"},{"location":"#highlights","title":"Highlights","text":"<ul> <li>JSON ingest and conversion to RDF using semantic annotations and conversions to a target model schema.</li> <li>entailment and validation pipeline for RDF resources.</li> <li>specific scripts to convert OGC source material into a form compatible with the OGC Linked Data environment</li> <li>tutorial for docker deployment and testing of available tools.</li> </ul>"},{"location":"#available-tools","title":"Available tools","text":"<p>The following tools are currently available:</p> <ul> <li><code>ingest_json</code>: Performs JSON-to-JSON-LD semantic uplifts (read more) </li> <li><code>update_vocabs</code>: Allows defining RDF entailment + validation + upload pipelines (read more)</li> <li><code>annotate_schema</code>: Annotates JSON schemas by leveraging <code>@modelReference</code> links to JSON-LD contexts (read more)</li> </ul>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#sample-domain-configuration","title":"Sample domain configuration","text":"<p>The following example shows a domain configuration in Turtle format:</p> <pre><code>@prefix dcfg: &lt;http://www.example.org/ogc/domain-cfg#&gt; .\n@prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; .\n@prefix dct: &lt;http://purl.org/dc/terms/&gt; .\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix profiles: &lt;http://www.opengis.net/def/metamodel/profiles/&gt; .\n\n_:OGC-NA-Catalog a dcat:Catalog ;\n  dct:title \"OGC Naming Authority catalog\" ;\n  rdfs:label \"OGC Naming Authority catalog\" ;\n\n  # Map http://defs-dev.opengis.net/ogc-na/x/y/z.ttl to \n  # ./x/y/z.ttl\n  dcfg:localArtifactMapping [\n    dcfg:baseURI \"http://defs-dev.opengis.net/ogc-na/\" ;\n    dcfg:localPath \"./\" ;\n  ] ;\n\n  # Link to enabled domain and uplift configurations\n  dcat:dataset _:conceptSchemes, _:semanticUplift ;\n\n  dcfg:hasProfileSource\n    \"sparql:https://example.org/sparql\",\n    \"path/to/profile.ttl\" ;\n.\n\n# A dcfg:DomainConfiguration will be used for semantic entailment and validation (update_vocabs)\n_:conceptSchemes a dcat:Dataset, dcfg:DomainConfiguration ;\n  dct:identifier \"conceptSchemes\" ;\n  dct:description \"Set of terms registered with OGC NA not covered by specialised domains\" ;\n\n  # Which files to include\n  dcfg:glob \"definitions/conceptSchemes/*.ttl\" ;\n\n  # URI root filter for detecting the main ConceptScheme in\n  # the source files\n  dcfg:uriRootFilter \"/def/\" ;\n\n  # Profile conformance can optionally be declared in the DomainConfiguration\n  # as well as in the source data itself\n  dct:conformsTo profiles:vocprez_ogc, profiles:skos_conceptscheme ;\n.\n\n# A dcfg:UpliftConfiguration will be used for semantic uplift (ingest_json)\n_:semanticUplift a dcat:Dataset, dcfg:UpliftConfiguration;\n  dct:identifier \"semanticUplift\" ;\n  dct:description \"Semantic uplift configuration\" ;\n\n  # Which files to include\n  dcfg:glob \"domain1/*.json\", \"domain2/*.json\" ;\n\n  # List of profiles (with semantic uplift artifacts) and/or files to \n  # use as uplift definitions\n  dcfg:hasUpliftDefinition\n    [ dcfg:order 1; dcfg:file \"path/to/file.yaml\" ],             # Local file, from working directory\n    [ dcfg:order 2; dcfg:profile profiles:vocprez_ogc ],         # Profile\n    [ dcfg:order 3; dcfg:file \"path/to/another/definition.yaml\"] # Local file, from working directory\n  ;\n.\n</code></pre>"},{"location":"examples/#sample-json-ld-uplift-context","title":"Sample JSON-LD uplift context","text":"<pre><code># Sample single-file JSON-LD context\n# Processing order is transform -&gt; types -&gt; context\n\n# Input filters allow reading files with a format other than JSON/JSON-LD.\n# Every filter has its own configuration options.\n# When using input filters, the output will be an object with two entries, \"metadata\" \n# (with metadata about the original file, the filter configuration, etc.), and \"data\"\n# (with the data read from the input document). \ninput-filter:\n  csv:\n    skip-rows: 2\n    trim-values: true\n\n\n# `path-scope` affects how ingest_json treats JSON-LD documents (e.g. when chaining uplifts).\n# It can be `graph` (transformations and paths act on `@graph`, if any, instead of on the\n# whole file) or `document` (do not treat JSON-LD files differently, process \"as is\").\n# Default is `graph`.\npath-scope: graph\n\n# `transform` uses jq expressions for light data transformations\n#   see: https://pypi.org/project/jq/\n#   see: https://stedolan.github.io/jq/manual/\n# This `transform` converts `\"key\": { ...value }` objects into an array\n#   with `[{ \"@id\" : \"#key\", ...value }, ...]` items, and also adds\n#   types `MyType` and `skos:Concept` to each of them\ntransform: '[to_entries[]|.value+{\"@id\":(\"#\"+.key),\"@type\":[\"MyType\", \"skos:Concept\"]}]'\n\n# `types` adds @type annotations to nodes represented by a jsonpath-ng expression\n# (note: expressions are matched against *transformed* data)\n#   see: https://pypi.org/project/jsonpath-ng/\ntypes:\n  '$[?type=\"IS\"]': [AddedClass, ISClass]\n  '$[?type=\"DP\"]': [AddedClass, DPClass]\n\n# `base-uri` sets the base uri that will be used for JSON-LD.\n#  This is sometimes necessary since pyld ignores the @base in the root @context\n#   see: https://github.com/digitalbazaar/pyld/issues/143\nbase-uri: http://example.org/vocab#\n\n# `context` adds JSON-LD @context to the root element (empty key, '.' or '$')\n# or to specific elements using jsonpath-ng expressions (note: expressions are\n# matched against *transformed* data\n#   see: https://pypi.org/project/jsonpath-ng/\ncontext:\n  # global context\n  '$': [\n    # dcterms profile\n    \"http://defs-dev.opengis.net/ogc-na/definitions/profiles/resources/dcterms.jsonld\",\n    # skos profile\n    \"http://defs-dev.opengis.net/ogc-na/definitions/profiles/resources/skos.jsonld\",\n\n    # custom context for bibliography\n    {\n      \"skos\": \"http://www.w3.org/2004/02/skos/core#\",\n      \"@vocab\": \"http://example.org/vocab#\",\n      \"type\": \"http://www.opengis.net/def/metamodel/ogc-na/doctype\",\n      \"alternative\": \"skos:altLabel\",\n      \"title\": \"skos:definition\",\n      \"description\": \"rdfs:comment\",\n      \"date\": \"dct:created\",\n      \"URL\": \"rdfs:seeAlso\"\n    }\n  ]\n\n  # scoped context for elements with \"type\" = \"IS\"\n  '$[?type=\"IS\"]': {\n    \"@vocab\": \"http://example.org/vocab3#\"\n  }\n\n# `context-position` dictates where the new context will be added if `@context` is already present\n# at any of the specified paths. Can be `before` (a new entry, with lower precedence, will be preprended\n# to any existing `@context`; this is the default behavior) or `after` (a new entry, with higher \n# precedence, will be appended to any existing `@context`). It has no effect for plain JSON documents. \ncontext-position: before\n</code></pre>"},{"location":"jsonld-uplift/","title":"JSON-LD Uplift","text":""},{"location":"jsonld-uplift/#input-filters","title":"Input filters","text":"<p>JSON-LD uplift (which is performed by the <code>ogc.na.ingest_json</code> module) not only accepts JSON/JSON-LD documents but also provides additional input filters to read other types of formats.</p> <p>The following input filters are available:</p> <ul> <li>CSV, for CSV and TSV files.</li> <li>XML, for XML documents.</li> </ul> <p>The filters generate a JSON-compatible version of the input files that can be then passed through the rest of the uplift steps. Configuring a filter is as simple as adding it to the <code>input-filter</code> section of the uplift definition:</p> <pre><code>input-filter:\n  csv:\n</code></pre> <p>Additional configuration options can be provided for the filter; for example, if we are working with a tab-separated file:</p> <pre><code>input-filter:\n  csv:\n    delimiter: '\\t'\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/#how-to-create-a-json-ld-uplift-context-definition","title":"How to create a JSON-LD uplift context definition","text":"<p><code>ogc.na.ingest_json</code>, the JSON-LD uplift module, can be used to convert JSON documents to JSON-LD, specially when those conversions are not straighforward (i.e., it is hard or even impossible to define a 1:1 relationship between the JSON structure and the desired RDF). <code>ingest_json</code> uses a JSON document and a JSON-LD uplift context definition  as its input and generates 2 outputs:</p> <ul> <li>An uplifted JSON-LD version of the input after applying transformations, classes and contexts.</li> <li>An RDF Graph with the parsed JSON-LD.</li> </ul> <p>JSON-LD uplift context definitions are created using YAML. YAML is a superset of JSON, making it easy to convert YAML to JSON (e.g., for a JSON-LD <code>@context</code> block), while at the same time several offering user-friendly features (such as comments or easy-to-read multiline text blocks).</p> <p>An uplift context definition is an object with 4 keys:</p> <ul> <li><code>transform</code> can either be a single jq expression or a list of those. jq is a sed-like tool for parsing and manipulating JSON.  The jq play tool is very useful to test jq transformations.</li> <li><code>types</code> is a map of jsonpath-ng paths to RDF classes. This is a convenience option to easily add types instead of having to  do so by using the more cumbersome <code>transform</code> above. <code>types</code> entries will be applied after the <code>transform</code>s, not on the original JSON document.</li> <li><code>base</code> is an optional string to define the base RDF URI that will be used for the output.</li> <li><code>context</code> is a map of jsonpath-ng path to JSON-LD <code>@context</code>. It will be used to inject <code>@context</code> blocks either globally or for specific nodes. <code>context</code> is applied after <code>transform</code> and <code>types</code>.</li> </ul> <p>If there is more than one <code>transform</code> entry, they will be chained (the output for the first one will be used as the input for the second one, and so on).  </p> <p>The special jsonpath-ng paths <code>$</code> and <code>.</code> can be used (in <code>types</code> and <code>context</code>) to refer to  the root object.</p> <p>All entries are optional (in the extreme case of an empty definition, no operations wil be performed on the input JSON document, and it will be parsed as JSON-LD as is). </p> <p>Let us start with a sample JSON document with the following content:</p> <pre><code>{\n  \"job1\": {\n    \"label\": \"Develop software\",\n    \"author\": \"Doe, John\",\n    \"status\": \"done\"\n  },\n  \"job2\": {\n    \"label\": \"Deploy production version\",\n    \"author\": \"Smith, Jane\",\n    \"status\": \"in-progress\"\n  }\n}\n</code></pre> <p>We want to convert it to RDF by:</p> <ol> <li>Assigning URIs to the job identifiers (their key) in the <code>http://example.com/job/&lt;id&gt;</code> form.</li> <li>Making the <code>author</code>s into <code>foaf:Person</code> with their respective <code>foaf:name</code>.</li> <li>Using <code>rdf:label</code> and <code>dc:creator</code> for <code>label</code> and <code>author</code>, respectively.</li> <li>Using the <code>http://example.com/status#</code> vocabulary for <code>status</code>es.</li> </ol>"},{"location":"tutorials/#assigning-uris-to-jobs","title":"Assigning URIs to jobs","text":"<p>We will start by turning the key -&gt; value mapping into an array, adding the key as <code>@id</code>. This can be achieved with a <code>transform</code>:</p> <pre><code>transform:\n  - '[to_entries | .[] | {\"@id\": .key} + .value]'\n</code></pre> <p><code>to_entries</code> will transform the key -&gt; value mappings into <code>{\"key\": \"&lt;key&gt;\", \"value\": { ... }}</code> objects. We can then use the <code>.[]</code> object iterator to visit each object, and then convert it into its value plus the <code>@id</code>. We wrap the  result in <code>[</code> and <code>]</code> so that the result is an array.</p> <p>We can then set the <code>@base</code> that will be used in the transform:</p> <pre><code>base: 'http://example.com/job/'\n</code></pre>"},{"location":"tutorials/#converting-authors","title":"Converting authors","text":"<p>This can be achieved with another jq <code>transform</code>:</p> <pre><code>transform:\n  - '[to_entries | .[] | {\"@id\": .key} + .value]'\n  - '[.[] | .author = {\"name\": .author}]' \n</code></pre> <p>This converts the string <code>author</code> field into an object with a <code>name</code> property containing its previous value. We then add a type, by using a path that searches for all <code>author</code> descendants of the root object:</p> <pre><code>types:\n  '$..author': 'Person'\n</code></pre>"},{"location":"tutorials/#adding-the-context","title":"Adding the @context","text":"<p>Finally, we add the necessary JSON-LD context for properties. The resulting full context definition is:</p> <pre><code>transform:\n  - '[to_entries | .[] | {\"@id\": .key} + .value]'\n  - '[.[] | .author = {\"name\": .author}]'\n\ntypes:\n  '$..author': 'Person'\n\nbase: 'http://example.com/job/'\n\ncontext:\n  '$':\n    '@base': 'http://example.com/job/'\n    rdfs: 'http://www.w3.org/2000/01/rdf-schema#'\n    foaf: 'http://xmlns.com/foaf/0.1/'\n    dc: 'http://purl.org/dc/elements/1.1/'\n    statusvoc: 'http://example.com/status#'\n    label: 'rdfs:label'\n    author: 'dc:creator'\n    name: 'foaf:name'\n    Person: 'foaf:person'\n    status:\n      '@id': 'statusvoc:status'\n      '@type': '@vocab'\n      '@context':\n        '@vocab': 'http://example.com/status#'\n</code></pre> <p>Which, after applying it to our input document, is converted into the following Turtle (provenance metadata, using the PROV-O ontology, is automatically added):</p> <pre><code>@prefix dc: &lt;http://purl.org/dc/elements/1.1/&gt; .\n@prefix dct: &lt;http://purl.org/dc/terms/&gt; .\n@prefix ns1: &lt;http://xmlns.com/foaf/0.1/&gt; .\n@prefix ns2: &lt;http://example.com/status#&gt; .\n@prefix prov: &lt;http://www.w3.org/ns/prov#&gt; .\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .\n\n&lt;http://example.com/job/job1&gt; rdfs:label \"Develop software\" ;\n    ns2:status ns2:done ;\n    dc:creator [ a ns1:person ;\n            ns1:name \"Doe, John\" ] .\n\n&lt;http://example.com/job/job2&gt; rdfs:label \"Deploy production version\" ;\n    ns2:status ns2:in-progress ;\n    dc:creator [ a ns1:person ;\n            ns1:name \"Smith, Jane\" ] .\n\n[] a prov:Activity ;\n    prov:endedAtTime \"2023-01-19T09:58:25.108552\"^^xsd:dateTime ;\n    prov:startedAtTime \"2023-01-19T09:58:24.674490\"^^xsd:dateTime ;\n    prov:used [ a prov:Entity ;\n            rdfs:label \"JSON document\" ;\n            dct:format \"application/json\" ],\n        [ a prov:Entity ;\n            rdfs:label \"Context definition\" ;\n            dct:format \"text/yaml\" ] ;\n    prov:wasAssociatedWith [ a prov:Agent,\n                &lt;https://schema.org/SoftwareApplication&gt; ;\n            rdfs:label \"OGC-NA tools\" ;\n            dct:hasVersion \"0.1.dev42+geab204d\" ;\n            rdfs:seeAlso &lt;https://github.com/opengeospatial/ogc-na-tools&gt; ] .\n</code></pre> <p>Alternatively, for the <code>author</code> type, we could have used a <code>transform</code> (adding the <code>@type</code> property to the second entry) or used a scoped <code>@context</code> (using <code>$..author</code> as the <code>context</code> key).</p>"},{"location":"tutorials/#chaining-json-ld-uplifts","title":"Chaining JSON-LD uplifts","text":"<p><code>ingest_json</code> can also work with already uplifted JSON-LD documents:</p> <ul> <li>If the root node has a <code>@graph</code> property, all transformations (jq operations) and paths will be anchored to it instead of the root node itself. If this is not desired, <code>path-scope: document</code> can be declared in the uplift definition.</li> <li>When injecting <code>context</code>s, if an existing JSON-LD <code>@context</code> in the node, the new context will be either prepended (by default) or appended to it; this can be controlled by adding a new uplift property <code>context-position</code> with the value <code>before</code> or <code>after</code>, respectively. Note that prepended context will have lower precedence than appended context.</li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>ogc.na<ul> <li>_version</li> <li>annotate_schema</li> <li>domain_config</li> <li>download</li> <li>gsp</li> <li>ingest_json</li> <li>input_filters<ul> <li>csv</li> <li>xml</li> </ul> </li> <li>models</li> <li>profile</li> <li>provenance</li> <li>update_vocabs</li> <li>util</li> <li>validation</li> </ul> </li> </ul>"},{"location":"reference/ogc/na/","title":"ogc.na","text":"<p>This module contains all the Naming Authority tools and libraries.</p>"},{"location":"reference/ogc/na/_version/","title":"_version","text":""},{"location":"reference/ogc/na/annotate_schema/","title":"annotate_schema","text":"<p>This module offers functionality to semantically enrich JSON Schemas by using <code>x-jsonld-context</code> annotations pointing to JSON-LD context documents, and also to build ready-to-use JSON-LD contexts from annotated JSON Schemas.</p> <p>An example of an annotated JSON schema:</p> <pre><code>\"$schema\": https://json-schema.org/draft/2020-12/schema\n\"x-jsonld-context\": observation.context.jsonld\ntitle: Observation\ntype: object\nrequired:\n  - featureOfInterest\n  - hasResult\n  - resultTime\nproperties:\n  featureOfInterest:\n    type: string\n  hasResult:\n    type: object\n  resultTime:\n    type: string\n    format: date-time\n  observationCollection:\n    type: string\n</code></pre> <p>... and its linked <code>x-jsonld-context</code>:</p> <pre><code>{\n  \"@context\": {\n    \"@version\": 1.1,\n    \"sosa\": \"http://www.w3.org/ns/sosa/\",\n    \"featureOfInterest\": \"sosa:featureOfInterest\",\n    \"hasResult\": \"sosa:hasResult\",\n    \"resultTime\": \"sosa:resultTime\"\n  }\n}\n</code></pre> <p>A <code>SchemaAnnotator</code> instance would then generate the following annotated JSON schema:</p> <pre><code>$schema: https://json-schema.org/draft/2020-12/schema\ntitle: Observation\ntype: object\nrequired:\n- featureOfInterest\n- hasResult\n- observationTime\nproperties:\n  featureOfInterest:\n    'x-jsonld-id': http://www.w3.org/ns/sosa/featureOfInterest\n    type: string\n  hasResult:\n    'x-jsonld-id': http://www.w3.org/ns/sosa/hasResult\n    type: object\n  observationCollection:\n    type: string\n  observationTime:\n    'x-jsonld-id': http://www.w3.org/ns/sosa/resultTime\n    format: date-time\n    type: string\n</code></pre> <p>This schema can then be referenced from other entities that follow it (e.g., by using FG-JSON \"definedby\" links).</p> <p>A client can then build a full JSON-LD <code>@context</code> (by using a <code>ContextBuilder</code> instance) and use it when parsing plain-JSON entities:</p> <pre><code>{\n  \"@context\": {\n    \"featureOfInterest\": \"http://www.w3.org/ns/sosa/featureOfInterest\",\n    \"hasResult\": \"http://www.w3.org/ns/sosa/hasResult\",\n    \"observationTime\": \"http://www.w3.org/ns/sosa/resultTime\"\n  }\n}\n</code></pre> <p>A JSON schema can be in YAML or JSON format (the annotated schema will use the same format as the input one).</p> <p>JSON schemas need to follow some rules to work with this tool:</p> <ul> <li>No nested <code>properties</code> are allowed. If they are needed, they should be put in a different schema, and a <code>$ref</code> to it used inside the appropriate property definition.</li> <li><code>allOf</code>/<code>someOf</code> root properties can be used to import other schemas (as long as they contain <code>$ref</code>s to them).</li> </ul> <p>This module can be run as a script, both for schema annotation and for context generation.</p> <p>To annotate a schema (that already contains a <code>x-jsonld-context</code> to a JSON-LD context resource):</p> <pre><code>python -m ogc.na.annotate_schema --file path/to/schema.file.yaml\n</code></pre> <p>This will generate a new <code>annotated</code> directory replicating the layout of the input file path (<code>/annotated/path/to/schema.file.yaml</code> in this example).</p> <p>JSON-LD contexts can be built by adding a <code>-c</code> flag:</p> <pre><code>python -m ogc.na.annotate_schema -c --file annotated/path/to/schema.file.yaml\n</code></pre> <p>The resulting context will be printed to the standard output.</p>"},{"location":"reference/ogc/na/annotate_schema/#ogc.na.annotate_schema.ContextBuilder","title":"<code>ContextBuilder</code>","text":"<p>Builds a JSON-LD context from a set of annotated JSON schemas.</p> Source code in <code>ogc/na/annotate_schema.py</code> <pre><code>class ContextBuilder:\n    \"\"\"\n    Builds a JSON-LD context from a set of annotated JSON schemas.\n    \"\"\"\n\n    def __init__(self, location: Path | str = None,\n                 compact: bool = True, ref_mapper: Callable[[str], str] | None = None,\n                 version=1.1):\n        \"\"\"\n        :param location: file or URL load the annotated schema from\n        :param compact: whether to compact the resulting context (remove redundancies, compact CURIEs)\n        :ref_mapper: an optional function to map JSON `$ref`'s before resolving them\n        \"\"\"\n        self.context = {'@context': {}}\n        self._parsed_schemas: dict[str | Path, dict] = {}\n        self._ref_mapper = ref_mapper\n\n        self._resolver = SchemaResolver()\n\n        self.location = location\n\n        self.visited_properties: dict[str, str | None] = {}\n        context = self._build_context(self.location, compact)\n        if context:\n            context['@version'] = version\n        self.context = {'@context': context}\n\n    def _build_context(self, schema_location: str | Path,\n                       compact: bool = True) -&gt; dict:\n\n        parsed = self._parsed_schemas.get(schema_location)\n        if parsed:\n            return parsed\n\n        root_schema = self._resolver.resolve_schema(schema_location)\n\n        prefixes = {}\n\n        own_context = {}\n\n        if prefixes:\n            own_context.update(prefixes)\n\n        def read_properties(subschema: dict, from_schema: ReferencedSchema,\n                            onto_context: dict, schema_path: list[str]) -&gt; dict | None:\n            if schema_path:\n                schema_path_str = '/' + '/'.join(schema_path)\n            else:\n                schema_path_str = ''\n            if not isinstance(subschema, dict):\n                return None\n            if subschema.get('type', 'object') != 'object':\n                return None\n            for prop, prop_val in subschema.get('properties', {}).items():\n                full_property_path = schema_path + [prop]\n                full_property_path_str = f\"{schema_path_str}/{prop}\"\n                self.visited_properties.setdefault(full_property_path_str, None)\n                if not isinstance(prop_val, dict):\n                    continue\n                prop_context = {'@context': {}}\n                for term, term_val in prop_val.items():\n                    if term == ANNOTATION_BASE:\n                        prop_context.setdefault('@context', {})['@base'] = term_val\n                    elif term.startswith(ANNOTATION_PREFIX) and term not in ANNOTATION_IGNORE_EXPAND:\n                        if term == ANNOTATION_ID:\n                            self.visited_properties[full_property_path_str] = term_val\n                        prop_context['@' + term[len(ANNOTATION_PREFIX):]] = term_val\n\n                if isinstance(prop_context.get('@id'), str):\n                    self.visited_properties[full_property_path_str] = prop_context['@id']\n                    if prop_context['@id'] == '@nest':\n                        process_subschema(prop_val, from_schema, onto_context, full_property_path)\n                    else:\n                        process_subschema(prop_val, from_schema, prop_context['@context'], full_property_path)\n                    if prop not in onto_context or isinstance(onto_context[prop], str):\n                        onto_context[prop] = prop_context\n                    else:\n                        merge_contexts(onto_context[prop], prop_context)\n                else:\n                    process_subschema(prop_val, from_schema, onto_context, full_property_path)\n\n        imported_prefixes: dict[str | Path, dict[str, str]] = {}\n        imported_extra_terms: dict[str | Path, dict[str, str]] = {}\n\n        def process_subschema(subschema: dict, from_schema: ReferencedSchema, onto_context: dict,\n                              schema_path: list[str]) -&gt; dict | None:\n\n            if not isinstance(subschema, dict):\n                return None\n\n            read_properties(subschema, from_schema, onto_context, schema_path)\n\n            if '$ref' in subschema:\n                ref = subschema['$ref']\n                if self._ref_mapper:\n                    ref = self._ref_mapper(ref)\n                referenced_schema = self._resolver.resolve_schema(ref, from_schema)\n                if referenced_schema:\n                    process_subschema(referenced_schema.subschema, referenced_schema, onto_context,\n                                      schema_path)\n\n            for i in ('allOf', 'anyOf', 'oneOf'):\n                l = subschema.get(i)\n                if isinstance(l, list):\n                    for idx, sub_subschema in enumerate(l):\n                        process_subschema(sub_subschema, from_schema, onto_context,\n                                          schema_path)\n\n            for i in ('prefixItems', 'items', 'contains', 'then', 'else', 'additionalProperties'):\n                l = subschema.get(i)\n                if isinstance(l, dict):\n                    process_subschema(l, from_schema, onto_context, schema_path)\n\n            for pp_k, pp in subschema.get('patternProperties', {}).items():\n                if isinstance(pp, dict):\n                    process_subschema(pp, from_schema, onto_context, schema_path + [pp_k])\n\n            if ANNOTATION_EXTRA_TERMS in subschema:\n                for extra_term, extra_term_context in subschema[ANNOTATION_EXTRA_TERMS].items():\n                    if extra_term not in onto_context:\n                        if isinstance(extra_term_context, dict):\n                            extra_term_context = {f\"@{k[len(ANNOTATION_PREFIX):]}\": v\n                                                  for k, v in extra_term_context.items()}\n                        onto_context[extra_term] = extra_term_context\n\n            if from_schema:\n                current_ref = f\"{from_schema.location}{from_schema.ref}\"\n                if current_ref not in imported_prefixes:\n                    sub_prefixes = subschema.get(ANNOTATION_PREFIXES, {})\n                    sub_prefixes |= from_schema.full_contents.get(ANNOTATION_PREFIXES, {})\n                    if sub_prefixes:\n                        imported_prefixes[current_ref] = sub_prefixes\n\n                if current_ref not in imported_extra_terms:\n                    sub_extra_terms = from_schema.full_contents.get(ANNOTATION_EXTRA_TERMS)\n                    if sub_extra_terms:\n                        imported_extra_terms[current_ref] = sub_extra_terms\n            else:\n                sub_prefixes = subschema.get(ANNOTATION_PREFIXES)\n                if isinstance(sub_prefixes, dict):\n                    prefixes.update({k: v for k, v in sub_prefixes.items() if k not in prefixes})\n\n        process_subschema(root_schema.subschema, root_schema, own_context, [])\n\n        for imported_et in imported_extra_terms.values():\n            for term, v in imported_et.items():\n                if term not in own_context:\n                    if isinstance(v, dict):\n                        v = {f\"@{k[len(ANNOTATION_PREFIX):]}\": val for k, val in v.items()}\n                    own_context[term] = v\n\n        for imported_prefix in imported_prefixes.values():\n            for p, v in imported_prefix.items():\n                if p not in prefixes:\n                    prefixes[p] = v\n\n        for prefix in list(prefixes.keys()):\n            if prefix not in own_context:\n                own_context[prefix] = {'@id': prefixes[prefix]}\n            else:\n                del prefixes[prefix]\n\n        if compact:\n\n            def compact_uri(uri: str) -&gt; str:\n                if uri.startswith('@'):\n                    # JSON-LD keyword\n                    return uri\n\n                for pref, pref_uri in prefixes.items():\n                    if uri.startswith(pref_uri) and len(pref_uri) &lt; len(uri):\n                        local_part = uri[len(pref_uri):]\n                        if local_part.startswith('//'):\n                            return uri\n                        return f\"{pref}:{local_part}\"\n\n                return uri\n\n            def compact_branch(branch, context_stack=None) -&gt; bool:\n                child_context_stack = context_stack + [branch] if context_stack else [branch]\n                terms = list(k for k in branch.keys() if k[0] != '@')\n\n                changed = False\n                for term in terms:\n                    term_value = branch[term]\n\n                    if isinstance(term_value, dict) and '@context' in term_value:\n                        if not term_value['@context']:\n                            del term_value['@context']\n                        else:\n                            while True:\n                                if not compact_branch(term_value['@context'], child_context_stack):\n                                    break\n\n                    if context_stack:\n                        for ctx in context_stack:\n                            if term not in ctx:\n                                continue\n                            other = ctx[term]\n                            if isinstance(term_value, str):\n                                term_value = {'@id': term_value}\n                            if isinstance(other, str):\n                                other = {'@id': other}\n                            if dict_contains(other, term_value):\n                                del branch[term]\n                                changed = True\n                                break\n\n                return changed\n\n            def compact_uris(branch, context_stack=None):\n                child_context_stack = context_stack + [branch] if context_stack else [branch]\n                terms = list(k for k in branch.keys() if k[0] != '@')\n                for term in terms:\n                    term_value = branch.get(term)\n                    if isinstance(term_value, str):\n                        branch[term] = compact_uri(term_value)\n                    elif isinstance(term_value, dict):\n                        if '@id' in term_value:\n                            term_value['@id'] = compact_uri(term_value['@id'])\n                        if len(term_value) == 1 and '@id' in term_value:\n                            branch[term] = term_value['@id']\n                        elif '@context' in term_value:\n                            compact_uris(term_value['@context'], child_context_stack)\n\n            compact_branch(own_context)\n            compact_uris(own_context)\n\n        self._parsed_schemas[schema_location] = own_context\n        return own_context\n</code></pre>"},{"location":"reference/ogc/na/annotate_schema/#ogc.na.annotate_schema.ContextBuilder.__init__","title":"<code>__init__(location=None, compact=True, ref_mapper=None, version=1.1)</code>","text":"<p>:ref_mapper: an optional function to map JSON <code>$ref</code>'s before resolving them</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>Path | str</code> <p>file or URL load the annotated schema from</p> <code>None</code> <code>compact</code> <code>bool</code> <p>whether to compact the resulting context (remove redundancies, compact CURIEs)</p> <code>True</code> Source code in <code>ogc/na/annotate_schema.py</code> <pre><code>def __init__(self, location: Path | str = None,\n             compact: bool = True, ref_mapper: Callable[[str], str] | None = None,\n             version=1.1):\n    \"\"\"\n    :param location: file or URL load the annotated schema from\n    :param compact: whether to compact the resulting context (remove redundancies, compact CURIEs)\n    :ref_mapper: an optional function to map JSON `$ref`'s before resolving them\n    \"\"\"\n    self.context = {'@context': {}}\n    self._parsed_schemas: dict[str | Path, dict] = {}\n    self._ref_mapper = ref_mapper\n\n    self._resolver = SchemaResolver()\n\n    self.location = location\n\n    self.visited_properties: dict[str, str | None] = {}\n    context = self._build_context(self.location, compact)\n    if context:\n        context['@version'] = version\n    self.context = {'@context': context}\n</code></pre>"},{"location":"reference/ogc/na/annotate_schema/#ogc.na.annotate_schema.SchemaAnnotator","title":"<code>SchemaAnnotator</code>","text":"<p>Builds a set of annotated JSON schemas from a collection of input schemas that have <code>x-jsonld-context</code>s to JSON-LD context documents.</p> <p>The results will be stored in the <code>schemas</code> property (a dictionary of schema-path-or-url -&gt; AnnotatedSchema mappings).</p> Source code in <code>ogc/na/annotate_schema.py</code> <pre><code>class SchemaAnnotator:\n    \"\"\"\n    Builds a set of annotated JSON schemas from a collection of input schemas\n    that have `x-jsonld-context`s to JSON-LD context documents.\n\n    The results will be stored in the `schemas` property (a dictionary of\n    schema-path-or-url -&gt; AnnotatedSchema mappings).\n    \"\"\"\n\n    def __init__(self, ref_mapper: Callable[[str, Any], str] | None = None):\n        \"\"\"\n        :ref_mapper: an optional function to map JSON `$ref`'s before resolving them\n        \"\"\"\n        self._schema_resolver = SchemaResolver()\n        self._ref_mapper = ref_mapper\n\n    def process_schema(self, location: Path | str | None,\n                       default_context: str | Path | dict | None = None,\n                       contents: dict | None = None) -&gt; AnnotatedSchema | None:\n        resolved_schema = self._schema_resolver.resolve_schema(location)\n        if contents:\n            # overriden\n            schema = contents\n        else:\n            schema = resolved_schema.subschema\n\n        if all(x not in schema for x in ('schema', 'openapi')):\n            validate_schema(schema)\n\n        context_fn = schema.get(ANNOTATION_CONTEXT)\n        schema.pop(ANNOTATION_CONTEXT, None)\n\n        context = {}\n        prefixes = {}\n\n        if default_context and (context_fn != default_context\n                                or not (isinstance(context_fn, Path)\n                                        and isinstance(default_context, Path)\n                                        and default_context.resolve() == context_fn.resolve())):\n            # Only load the provided context if it's different from the schema-referenced one\n            resolved_default_context = resolve_context(default_context)\n            context, prefixes = attrgetter('context', 'prefixes')(resolved_default_context)\n\n        if context_fn:\n            context_fn, fragment = self._schema_resolver.resolve_ref(context_fn, resolved_schema)\n            schema_context = resolve_context(context_fn)\n\n            context = merge_contexts(context, schema_context.context)\n            prefixes = prefixes | schema_context.prefixes\n\n        updated_refs: set[int] = set()\n\n        def find_prop_context(prop, context_stack) -&gt; dict | None:\n            for ctx in reversed(context_stack):\n                vocab = ctx.get('@vocab')\n                if prop in ctx:\n                    prop_ctx = ctx[prop]\n                    if isinstance(prop_ctx, str):\n                        if vocab and ':' not in prop_ctx and prop_ctx[0] != '@':\n                            prop_ctx = f\"{vocab}{prop_ctx}\"\n                        return {'@id': prop_ctx}\n                    elif '@id' not in prop_ctx and not vocab:\n                        raise ValueError(f'Missing @id for property {prop} in context {json.dumps(ctx, indent=2)}')\n                    else:\n                        result = {k: v for k, v in prop_ctx.items() if k.startswith('@')}\n                        if vocab:\n                            prop_id = result.get('@id')\n                            if not prop_id:\n                                result['@id'] = f\"{vocab}{prop}\"\n                            elif ':' not in prop_id and prop_id[0] != '@':\n                                result['@id'] = f\"{vocab}{prop_id}\"\n                        return result\n                elif '@vocab' in ctx:\n                    return {'@id': f\"{ctx['@vocab']}{prop}\"}\n\n        def process_properties(obj: dict, context_stack: list[dict[str, Any]],\n                               from_schema: ReferencedSchema, level) -&gt; Iterable[str]:\n\n            properties: dict[str, dict] = obj.get('properties') if obj else None\n            if not properties:\n                return ()\n            if not isinstance(properties, dict):\n                raise ValueError('\"properties\" must be a dictionary')\n\n            used_terms = set()\n            for prop in list(properties.keys()):\n                if prop[0] == '@':\n                    # skip JSON-LD keywords\n                    continue\n                prop_value = properties[prop]\n                prop_ctx = find_prop_context(prop, context_stack)\n                if prop_ctx:\n                    used_terms.add(prop)\n                    prop_schema_ctx = {f\"{ANNOTATION_PREFIX}{k[1:]}\": v\n                                       for k, v in prop_ctx.items()\n                                       if k[0] == '@' and k != '@context'}\n                    prop_ctx_base = prop_ctx.get('@context', {}).get('@base')\n                    if prop_ctx_base:\n                        prop_schema_ctx[ANNOTATION_BASE] = prop_ctx_base\n\n                    if not prop_value or prop_value is True:\n                        properties[prop] = prop_schema_ctx\n                    else:\n                        for k, v in prop_schema_ctx.items():\n                            prop_value.setdefault(k, v)\n\n                if prop_ctx and '@context' in prop_ctx:\n                    prop_context_stack = context_stack + [prop_ctx['@context']]\n                else:\n                    prop_context_stack = context_stack\n                used_terms.update(process_subschema(prop_value, prop_context_stack, from_schema, level))\n\n            return used_terms\n\n        def process_subschema(subschema, context_stack, from_schema: ReferencedSchema, level=1) -&gt; Iterable[str]:\n            if not subschema or not isinstance(subschema, dict):\n                return ()\n\n            used_terms = set()\n\n            if '$ref' in subschema and id(subschema) not in updated_refs:\n                if self._ref_mapper:\n                    subschema['$ref'] = self._ref_mapper(subschema['$ref'], subschema)\n                if subschema['$ref'].startswith('#/') or subschema['$ref'].startswith(f\"{from_schema.location}#/\"):\n                    target_schema = self._schema_resolver.resolve_schema(subschema['$ref'], from_schema)\n                    if target_schema:\n                        used_terms.update(process_subschema(target_schema.subschema, context_stack,\n                                                            target_schema, level + 1))\n                updated_refs.add(id(subschema))\n\n            # Annotate oneOf, allOf, anyOf\n            for p in ('oneOf', 'allOf', 'anyOf'):\n                collection = subschema.get(p)\n                if collection and isinstance(collection, list):\n                    for entry in collection:\n                        used_terms.update(process_subschema(entry, context_stack, from_schema, level + 1))\n\n            for p in ('then', 'else', 'additionalProperties'):\n                branch = subschema.get(p)\n                if branch and isinstance(branch, dict):\n                    used_terms.update(process_subschema(branch, context_stack, from_schema, level))\n\n            for pp in subschema.get('patternProperties', {}).values():\n                if pp and isinstance(pp, dict):\n                    used_terms.update(process_subschema(pp, context_stack, from_schema, level + 1))\n\n            # Annotate main schema\n            schema_type = subschema.get('type')\n            if not schema_type and 'properties' in subschema:\n                schema_type = 'object'\n\n            if schema_type == 'object':\n                used_terms.update(process_properties(subschema, context_stack, from_schema, level + 1))\n            elif schema_type == 'array':\n                for k in ('prefixItems', 'items', 'contains'):\n                    used_terms.update(process_subschema(subschema.get(k), context_stack, from_schema, level + 1))\n\n            # Get prefixes\n            for p, bu in subschema.get(ANNOTATION_PREFIXES, {}).items():\n                if p not in prefixes:\n                    prefixes[p] = bu\n\n            if len(context_stack) == level and context_stack[-1]:\n                extra_terms = {}\n                for k, v in context_stack[-1].items():\n                    if k[0] != '@' and k not in prefixes and k not in used_terms:\n                        if isinstance(v, dict):\n                            if len(v) == 1 and '@id' in v:\n                                v = v['@id']\n                            else:\n                                v = {f\"{ANNOTATION_PREFIX}{vk[1:]}\": vv for vk, vv in v.items() if vk[0] == '@'}\n                        if isinstance(v, str) and v[-1] in ('#', '/', ':'):\n                            prefixes[k] = v\n                        else:\n                            extra_terms[k] = v\n                if extra_terms:\n                    subschema.setdefault(ANNOTATION_EXTRA_TERMS, {}).update(extra_terms)\n\n            return used_terms\n\n        process_subschema(schema, [context], resolved_schema)\n\n        if prefixes:\n            schema[ANNOTATION_PREFIXES] = prefixes\n\n        return AnnotatedSchema(\n            source=location,\n            is_json=resolved_schema.is_json,\n            schema=schema\n        )\n</code></pre>"},{"location":"reference/ogc/na/annotate_schema/#ogc.na.annotate_schema.SchemaAnnotator.__init__","title":"<code>__init__(ref_mapper=None)</code>","text":"<p>:ref_mapper: an optional function to map JSON <code>$ref</code>'s before resolving them</p> Source code in <code>ogc/na/annotate_schema.py</code> <pre><code>def __init__(self, ref_mapper: Callable[[str, Any], str] | None = None):\n    \"\"\"\n    :ref_mapper: an optional function to map JSON `$ref`'s before resolving them\n    \"\"\"\n    self._schema_resolver = SchemaResolver()\n    self._ref_mapper = ref_mapper\n</code></pre>"},{"location":"reference/ogc/na/annotate_schema/#ogc.na.annotate_schema.dump_annotated_schema","title":"<code>dump_annotated_schema(schema, subdir='annotated', root_dir=None, output_fn_transform=None)</code>","text":"<p>Creates a \"mirror\" directory (named <code>annotated</code> by default) with the resulting schemas annotated by a <code>SchemaAnnotator</code>.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>AnnotatedSchema</code> <p>the <code>AnnotatedSchema</code> to dump</p> required <code>subdir</code> <code>Path | str</code> <p>a name for the mirror directory</p> <code>'annotated'</code> <code>root_dir</code> <code>Path | str | None</code> <p>root directory for computing relative paths to schemas</p> <code>None</code> <code>output_fn_transform</code> <code>Callable[[Path], Path] | None</code> <p>optional callable to transform the output path</p> <code>None</code> Source code in <code>ogc/na/annotate_schema.py</code> <pre><code>def dump_annotated_schema(schema: AnnotatedSchema, subdir: Path | str = 'annotated',\n                          root_dir: Path | str | None = None,\n                          output_fn_transform: Callable[[Path], Path] | None = None) -&gt; None:\n    \"\"\"\n    Creates a \"mirror\" directory (named `annotated` by default) with the resulting\n    schemas annotated by a `SchemaAnnotator`.\n\n    :param schema: the `AnnotatedSchema` to dump\n    :param subdir: a name for the mirror directory\n    :param root_dir: root directory for computing relative paths to schemas\n    :param output_fn_transform: optional callable to transform the output path\n    \"\"\"\n    wd = (Path(root_dir) if root_dir else Path()).resolve()\n    subdir = subdir if isinstance(subdir, Path) else Path(subdir)\n    path = schema.source\n    if isinstance(path, Path):\n        output_fn = path.resolve().relative_to(wd)\n    else:\n        parsed = urlparse(str(path))\n        output_fn = parsed.path\n\n    output_fn = subdir / output_fn\n    if output_fn_transform:\n        output_fn = output_fn_transform(output_fn)\n    output_fn.parent.mkdir(parents=True, exist_ok=True)\n\n    if schema.is_json:\n        logger.info(f'Writing output schema to {output_fn}')\n        with open(output_fn, 'w') as f:\n            json.dump(schema.schema, f, indent=2)\n    else:\n        dump_yaml(schema.schema, output_fn)\n</code></pre>"},{"location":"reference/ogc/na/annotate_schema/#ogc.na.annotate_schema.load_json_yaml","title":"<code>load_json_yaml(contents)</code>","text":"<p>Loads either a JSON or a YAML file</p> <p>Parameters:</p> Name Type Description Default <code>contents</code> <code>str | bytes</code> <p>contents to load</p> required <p>Returns:</p> Type Description <code>tuple[Any, bool]</code> <p>a tuple with the loaded document, and whether the detected format was JSON (True) or YAML (False)</p> Source code in <code>ogc/na/annotate_schema.py</code> <pre><code>def load_json_yaml(contents: str | bytes) -&gt; tuple[Any, bool]:\n    \"\"\"\n    Loads either a JSON or a YAML file\n\n    :param contents: contents to load\n    :return: a tuple with the loaded document, and whether the detected format was JSON (True) or YAML (False)\n    \"\"\"\n    try:\n        obj = json.loads(contents)\n        is_json = True\n    except ValueError:\n        obj = load_yaml(content=contents)\n        is_json = False\n\n    return obj, is_json\n</code></pre>"},{"location":"reference/ogc/na/annotate_schema/#ogc.na.annotate_schema.read_contents","title":"<code>read_contents(location)</code>","text":"<p>Reads contents from a file or URL</p> <p>@param location: filename or URL to load @return: a tuple with the loaded data (str or bytes) and the base URL, if any</p> Source code in <code>ogc/na/annotate_schema.py</code> <pre><code>def read_contents(location: Path | str | None) -&gt; tuple[AnyStr | bytes, str]:\n    \"\"\"\n    Reads contents from a file or URL\n\n    @param location: filename or URL to load\n    @return: a tuple with the loaded data (str or bytes) and the base URL, if any\n    \"\"\"\n    if not location:\n        raise ValueError('A location must be provided')\n\n    if isinstance(location, Path) or not is_url(location):\n        fn = Path(location)\n        base_url = None\n        logger.info('Reading file contents from %s', fn)\n        with open(fn) as f:\n            contents = f.read()\n    else:\n        base_url = location\n        r = requests_session.get(location)\n        r.raise_for_status()\n        contents = r.content\n\n    return contents, base_url\n</code></pre>"},{"location":"reference/ogc/na/annotate_schema/#ogc.na.annotate_schema.resolve_ref","title":"<code>resolve_ref(ref, fn_from=None, url_from=None, base_url=None)</code>","text":"<p>Resolves a <code>$ref</code></p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>str</code> <p>the <code>$ref</code> to resolve</p> required <code>fn_from</code> <code>str | Path | None</code> <p>original name of the file containing the <code>$ref</code> (when it is a file)</p> <code>None</code> <code>url_from</code> <code>str | None</code> <p>original URL of the document containing the <code>$ref</code> (when it is a URL)</p> <code>None</code> <code>base_url</code> <code>str | None</code> <p>base URL of the document containing the <code>$ref</code> (if any)</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Path | None, str | None]</code> <p>a tuple of (Path, str) with only one None entry (the Path if the resolved reference is a file, or the str if it is a URL)</p> Source code in <code>ogc/na/annotate_schema.py</code> <pre><code>def resolve_ref(ref: str, fn_from: str | Path | None = None, url_from: str | None = None,\n                base_url: str | None = None) -&gt; tuple[Path | None, str | None]:\n    \"\"\"\n    Resolves a `$ref`\n    :param ref: the `$ref` to resolve\n    :param fn_from: original name of the file containing the `$ref` (when it is a file)\n    :param url_from: original URL of the document containing the `$ref` (when it is a URL)\n    :param base_url: base URL of the document containing the `$ref` (if any)\n    :return: a tuple of (Path, str) with only one None entry (the Path if the resolved\n    reference is a file, or the str if it is a URL)\n    \"\"\"\n\n    base_url = base_url or url_from\n    if is_url(ref):\n        return None, ref\n    elif base_url:\n        return None, urljoin(base_url, ref)\n    else:\n        fn_from = fn_from if isinstance(fn_from, Path) else Path(fn_from)\n        ref = (fn_from.resolve().parent / ref).resolve()\n        return ref, None\n</code></pre>"},{"location":"reference/ogc/na/domain_config/","title":"domain_config","text":"<p>This module contains classes to load RDF domain configuration files (DCAT-like catalogs) defining how to find and select files for processing.</p>"},{"location":"reference/ogc/na/domain_config/#ogc.na.domain_config.ConfigurationEntryList","title":"<code>ConfigurationEntryList</code>","text":"<p>             Bases: <code>list[CE]</code></p> Source code in <code>ogc/na/domain_config.py</code> <pre><code>class ConfigurationEntryList(list[CE]):\n\n    def find_entry_for_file(self, fn: str | Path) -&gt; ConfigurationEntry | None:\n        \"\"\"\n        Find the configuration entry that corresponds to a file, if any.\n\n        :param fn: the file name\n        :return: a DomainConfigurationEntry, or None if none is found\n        \"\"\"\n        if not isinstance(fn, Path):\n            fn = Path(fn)\n\n        for entry in self:\n            if entry.matches(fn):\n                return entry\n\n    def find_entries_for_files(self, fns: list[str | Path]) -&gt; 'dict[Path, ConfigurationEntry]':\n        \"\"\"\n        Find the configuration entries associated to a list of files. Similar\n        to [find_entry_for_file()][ogc.na.domain_config.ConfigurationEntryList.find_entry_for_file]\n        but with a list of files.\n\n        :param fns: a list of files to find\n        :return: a path \\u2192 DomainConfigurationEntry dict for each file that is found\n        \"\"\"\n        result: dict[Path, ConfigurationEntry] = {}\n        for fn in fns:\n            p = Path(fn).resolve()\n            e = self.find_entry_for_file(p)\n            if e:\n                result[p] = e\n        return result\n\n    def find_all(self) -&gt; 'dict[Path, ConfigurationEntry]':\n        \"\"\"\n        Find all the files referenced by this configuration entry list, including\n        their DomainConfigurationEntry.\n\n        :return: a path to DomainConfigurationEntry mapping (dict) including all files\n        \"\"\"\n        r = {}\n        for entry in self:\n            r.update({p: entry for p in entry.find_all()})\n        return r\n</code></pre>"},{"location":"reference/ogc/na/domain_config/#ogc.na.domain_config.ConfigurationEntryList.find_all","title":"<code>find_all()</code>","text":"<p>Find all the files referenced by this configuration entry list, including their DomainConfigurationEntry.</p> <p>Returns:</p> Type Description <code>'dict[Path, ConfigurationEntry]'</code> <p>a path to DomainConfigurationEntry mapping (dict) including all files</p> Source code in <code>ogc/na/domain_config.py</code> <pre><code>def find_all(self) -&gt; 'dict[Path, ConfigurationEntry]':\n    \"\"\"\n    Find all the files referenced by this configuration entry list, including\n    their DomainConfigurationEntry.\n\n    :return: a path to DomainConfigurationEntry mapping (dict) including all files\n    \"\"\"\n    r = {}\n    for entry in self:\n        r.update({p: entry for p in entry.find_all()})\n    return r\n</code></pre>"},{"location":"reference/ogc/na/domain_config/#ogc.na.domain_config.ConfigurationEntryList.find_entries_for_files","title":"<code>find_entries_for_files(fns)</code>","text":"<p>Find the configuration entries associated to a list of files. Similar to find_entry_for_file() but with a list of files.</p> <p>Parameters:</p> Name Type Description Default <code>fns</code> <code>list[str | Path]</code> <p>a list of files to find</p> required <p>Returns:</p> Type Description <code>'dict[Path, ConfigurationEntry]'</code> <p>a path \u2192 DomainConfigurationEntry dict for each file that is found</p> Source code in <code>ogc/na/domain_config.py</code> <pre><code>def find_entries_for_files(self, fns: list[str | Path]) -&gt; 'dict[Path, ConfigurationEntry]':\n    \"\"\"\n    Find the configuration entries associated to a list of files. Similar\n    to [find_entry_for_file()][ogc.na.domain_config.ConfigurationEntryList.find_entry_for_file]\n    but with a list of files.\n\n    :param fns: a list of files to find\n    :return: a path \\u2192 DomainConfigurationEntry dict for each file that is found\n    \"\"\"\n    result: dict[Path, ConfigurationEntry] = {}\n    for fn in fns:\n        p = Path(fn).resolve()\n        e = self.find_entry_for_file(p)\n        if e:\n            result[p] = e\n    return result\n</code></pre>"},{"location":"reference/ogc/na/domain_config/#ogc.na.domain_config.ConfigurationEntryList.find_entry_for_file","title":"<code>find_entry_for_file(fn)</code>","text":"<p>Find the configuration entry that corresponds to a file, if any.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str | Path</code> <p>the file name</p> required <p>Returns:</p> Type Description <code>ConfigurationEntry | None</code> <p>a DomainConfigurationEntry, or None if none is found</p> Source code in <code>ogc/na/domain_config.py</code> <pre><code>def find_entry_for_file(self, fn: str | Path) -&gt; ConfigurationEntry | None:\n    \"\"\"\n    Find the configuration entry that corresponds to a file, if any.\n\n    :param fn: the file name\n    :return: a DomainConfigurationEntry, or None if none is found\n    \"\"\"\n    if not isinstance(fn, Path):\n        fn = Path(fn)\n\n    for entry in self:\n        if entry.matches(fn):\n            return entry\n</code></pre>"},{"location":"reference/ogc/na/domain_config/#ogc.na.domain_config.DomainConfiguration","title":"<code>DomainConfiguration</code>","text":"<p>The DomainConfiguration class can load a collection of ConfigurationEntry's detailing which files need to be processed and where they can be found, as well as including a list of profiles for entailment, validation, and (potentially) other operations.</p> <p>Domain configurations use the <code>http://www.example.org/ogc/domain-cfg#</code> (dcfg) prefix.</p> <p>A domain configuration must include, at least, a <code>dcfg:glob</code> (glob expression to find/filter files inside the base directory). If present, a <code>dcfg:uriRootFilter</code> will be used to determine which is the main concept scheme in the file (if more than one is found). Profiles for validation, entailment, etc. can be specified using <code>dcterms:conformsTo</code>.</p> <p><code>dcfg:hasUpliftDefinition</code> can also be used to declare (ordered) semantic uplift definitions, either from profile artifacts or from files.</p> Source code in <code>ogc/na/domain_config.py</code> <pre><code>class DomainConfiguration:\n    \"\"\"\n    The DomainConfiguration class can load a collection of ConfigurationEntry's\n    detailing which files need to be processed and where they can be found, as well\n    as including a list of profiles for entailment, validation, and (potentially)\n    other operations.\n\n    Domain configurations use the `http://www.example.org/ogc/domain-cfg#` (dcfg) prefix.\n\n    A domain configuration must include, at least, a `dcfg:glob` (glob expression to find/filter\n    files inside the base directory). If present, a `dcfg:uriRootFilter` will be used to determine\n    which is the main concept scheme in the file (if more than one is found). Profiles for\n    validation, entailment, etc. can be specified using `dcterms:conformsTo`.\n\n    `dcfg:hasUpliftDefinition` can also be used to declare (ordered) semantic uplift definitions, either\n    from profile artifacts or from files.\n    \"\"\"\n\n    def __init__(self, source: Union[Graph, str, Path, IO], working_directory: str | Path = None,\n                 profile_sources: str | Path | Iterable[str | Path] | None = None,\n                 ignore_artifact_errors=False, local_artifacts_mappings: dict | None = None):\n        \"\"\"\n        Creates a new DomainConfiguration, optionally specifying the working directory.\n\n        :param source: Graph or Turtle file to load\n        :param working_directory: the working directory to use for local paths.\n        \"\"\"\n        if working_directory:\n            self.working_directory = Path(working_directory).resolve()\n        elif isinstance(source, str) or isinstance(source, Path):\n            self.working_directory = Path(source).parent.resolve()\n        else:\n            self.working_directory = Path().resolve()\n        logger.info(\"Working directory: %s\", self.working_directory)\n        self.entries = ConfigurationEntryList()\n        self.uplift_entries = UpliftConfigurationEntryList()\n        self.local_artifacts_mappings = {}\n        if local_artifacts_mappings:\n            self.local_artifacts_mappings.update(local_artifacts_mappings)\n        self.profile_registry: ProfileRegistry | None = None\n        self._profile_sources = profile_sources\n        self._ignore_artifact_errors = ignore_artifact_errors\n\n        self._load(source)\n\n    def _load(self, source: Union[Graph, str, IO]):\n        \"\"\"\n        Load entries from a Graph or Turtle document.\n\n        :param source: Graph or Turtle file to load\n        :return: this DomainConfiguration instance\n        \"\"\"\n        service = ''\n        if isinstance(source, Graph):\n            g = source\n        elif isinstance(source, str) and source.startswith('sparql:'):\n            service = source[len('sparql:'):]\n            g = Graph()\n        else:\n            g = Graph().parse(source)\n\n        cfg_graph = g.query(DOMAIN_CFG_QUERY.replace('__SERVICE__', service)).graph\n\n        ignore_profile_artifact_errors = self._ignore_artifact_errors\n\n        prof_sources: set[str | Path] = set()\n        for catalog_ref in cfg_graph.subjects(DCAT.dataset):\n            logger.debug(\"Found catalog %s\", catalog_ref)\n\n            if bool(cfg_graph.value(catalog_ref, DCFG.ignoreProfileArtifactErrors)):\n                ignore_profile_artifact_errors = True\n\n            # Local artifacts mapping\n            for mapping_ref in cfg_graph.objects(catalog_ref, DCFG.localArtifactMapping):\n                base_uri = str(cfg_graph.value(mapping_ref, DCFG.baseURI))\n                if base_uri in self.local_artifacts_mappings:\n                    logger.debug(\"Local artifact mapping for %s overriden\", base_uri)\n                    # Overriden\n                    continue\n                local_path = Path(str(cfg_graph.value(mapping_ref, DCFG.localPath)))\n                logger.debug(\"Found local artifact mapping: %s -&gt; %s\", base_uri, local_path)\n                self.local_artifacts_mappings[base_uri] = local_path\n\n            # Profile sources\n            for p in cfg_graph.objects(catalog_ref, DCFG.hasProfileSource):\n                if not isinstance(p, Literal):\n                    continue\n                if p.value.startswith('sparql:'):\n                    prof_sources.add(p.value)\n                else:\n                    prof_sources.update(self.working_directory.glob(p.value))\n\n            if self._profile_sources:\n                prof_sources.update(self._profile_sources)\n\n        self.profile_registry = ProfileRegistry(prof_sources,\n                                                ignore_artifact_errors=ignore_profile_artifact_errors,\n                                                local_artifact_mappings=self.local_artifacts_mappings)\n\n        for cfg_ref in cfg_graph.objects(predicate=DCAT.dataset):\n\n            globs = [str(g) for g in cfg_graph.objects(cfg_ref, DCFG.glob)]\n\n            # DomainConfigurationEntry specific properties\n            uri_root_filter = cfg_graph.value(cfg_ref, DCFG.uriRootFilter)\n            profile_refs = cast(list[URIRef], list(cfg_graph.objects(cfg_ref, DCTERMS.conformsTo)))\n\n            # UpliftConfigurationEntry specific properties\n            found_uplift_defs = []\n            max_order = None\n            for uplift_def_ref in cfg_graph.objects(cfg_ref, DCFG.hasUpliftDefinition):\n                order = cfg_graph.value(uplift_def_ref, DCFG.order)\n                if order is not None and (max_order is None or int(order) &gt; max_order):\n                    max_order = int(order)\n                target_prof = cfg_graph.value(uplift_def_ref, DCFG.profile)\n                target_file = cfg_graph.value(uplift_def_ref, DCFG.file)\n                if target_prof:\n                    found_uplift_defs.append([order, target_prof])\n                elif target_file:\n                    found_uplift_defs.append([order, self.working_directory.joinpath(str(target_file)).resolve()])\n            uplift_defs = [p[1] for p in\n                           sorted(found_uplift_defs,\n                                  key=lambda u: u[0] if u[0] is not None else max_order + 1)]\n\n            identifier = cfg_graph.value(cfg_ref, DCTERMS.identifier) or str(cfg_ref)\n\n            if profile_refs:\n                self.entries.append(DomainConfigurationEntry(\n                    working_directory=self.working_directory,\n                    glob=globs,\n                    identifier=identifier,\n                    uri_root_filter=uri_root_filter,\n                    conforms_to=profile_refs,\n                ))\n\n            if uplift_defs:\n                self.uplift_entries.append(UpliftConfigurationEntry(\n                    working_directory=self.working_directory,\n                    glob=globs,\n                    identifier=identifier,\n                    uplift_definitions=uplift_defs,\n                ))\n\n        logger.info(\"Found %d domain configurations and %d uplift configurations\",\n                    len(self.entries),\n                    len(self.uplift_entries))\n\n        return self\n\n    def __len__(self):\n        return len(self.entries) + len(self.uplift_entries)\n</code></pre>"},{"location":"reference/ogc/na/domain_config/#ogc.na.domain_config.DomainConfiguration.__init__","title":"<code>__init__(source, working_directory=None, profile_sources=None, ignore_artifact_errors=False, local_artifacts_mappings=None)</code>","text":"<p>Creates a new DomainConfiguration, optionally specifying the working directory.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[Graph, str, Path, IO]</code> <p>Graph or Turtle file to load</p> required <code>working_directory</code> <code>str | Path</code> <p>the working directory to use for local paths.</p> <code>None</code> Source code in <code>ogc/na/domain_config.py</code> <pre><code>def __init__(self, source: Union[Graph, str, Path, IO], working_directory: str | Path = None,\n             profile_sources: str | Path | Iterable[str | Path] | None = None,\n             ignore_artifact_errors=False, local_artifacts_mappings: dict | None = None):\n    \"\"\"\n    Creates a new DomainConfiguration, optionally specifying the working directory.\n\n    :param source: Graph or Turtle file to load\n    :param working_directory: the working directory to use for local paths.\n    \"\"\"\n    if working_directory:\n        self.working_directory = Path(working_directory).resolve()\n    elif isinstance(source, str) or isinstance(source, Path):\n        self.working_directory = Path(source).parent.resolve()\n    else:\n        self.working_directory = Path().resolve()\n    logger.info(\"Working directory: %s\", self.working_directory)\n    self.entries = ConfigurationEntryList()\n    self.uplift_entries = UpliftConfigurationEntryList()\n    self.local_artifacts_mappings = {}\n    if local_artifacts_mappings:\n        self.local_artifacts_mappings.update(local_artifacts_mappings)\n    self.profile_registry: ProfileRegistry | None = None\n    self._profile_sources = profile_sources\n    self._ignore_artifact_errors = ignore_artifact_errors\n\n    self._load(source)\n</code></pre>"},{"location":"reference/ogc/na/download/","title":"download","text":""},{"location":"reference/ogc/na/gsp/","title":"gsp","text":"<p>SPARQL Graph Store Protocol operations</p>"},{"location":"reference/ogc/na/gsp/#ogc.na.gsp.GraphStore","title":"<code>GraphStore</code>","text":"<p>Encapsulates Graph Store Protocol configuration for executing operations.</p> Source code in <code>ogc/na/gsp.py</code> <pre><code>class GraphStore:\n    \"\"\"\n    Encapsulates Graph Store Protocol configuration for executing operations.\n    \"\"\"\n\n    def __init__(self, url: str, auth_details: tuple[str, str] | None = None):\n        \"\"\"\n        Constructs a new GraphStore\n        :param url: SPARQL Graph Store Protocol URL\n        :param auth_details: tuple in the form ('username', 'password') for authentication\n        \"\"\"\n        self._url = url\n        self.auth_details = auth_details\n        self.put = self.replace\n        self.post = self.add\n\n    def _post_or_put(self, method: str, graph_uri, source: str | bytes | Path, format: str = 'text/turtle'):\n        if graph_uri:\n            params = {'graph': str(graph_uri)}\n        else:\n            params = 'default'\n\n        if isinstance(source, Path):\n            with open(source, 'r') as f:\n                data = f.read()\n        elif isinstance(source, IO) or isinstance(source, IOBase):\n            data = source.read()\n        else:\n            data = source\n\n        if isinstance(data, str):\n            data = data.encode('utf-8')\n\n        method = getattr(requests, method)\n        r = method(\n            self._url,\n            params=params,\n            headers={\n                'Content-type': format,\n            },\n            auth=self.auth_details,\n            data=data,\n        )\n\n        if isinstance(data, Generator):\n            data.close()\n\n        r.raise_for_status()\n\n    def delete(self, graph_uri: str | None, ignore_404=True):\n        \"\"\"\n        Deletes a graph from the Graph Store\n        :param graph_uri: URI for the graph (if None, the default graph will be used)\n        :param ignore_404: Whether to ignore HTTP 404 errors (otherwise, an Exception will be thrown)\n        :return:\n        \"\"\"\n        if graph_uri:\n            params = {'graph': str(graph_uri)}\n        else:\n            params = 'default'\n\n        r = requests.delete(\n            self._url,\n            params=params,\n            auth=self.auth_details,\n        )\n        if not (ignore_404 and r.status_code == 404):\n            r.raise_for_status()\n\n    def replace(self, graph_uri: str | None, source: str | bytes | Path, format: str = 'text/turtle'):\n        \"\"\"\n        Replaces the data for a Graph Store graph with the provided source (HTTP PUT operation)\n        :param graph_uri: URI for the graph (if None, the default graph will be used)\n        :param source: Source data or file name\n        :param format: Media type to provide to the Graph Store\n        :return:\n        \"\"\"\n        self._post_or_put('put', graph_uri, source, format)\n\n    def add(self, graph_uri: str | None, source: str | bytes | Path, format: str = 'text/turtle'):\n        \"\"\"\n        Adds data from the provided source to a Graph Store graph (HTTP PUT operation)\n        :param graph_uri: URI for the graph (if None, the default graph will be used)\n        :param source: Source data or file name\n        :param format: Media type to provide to the Graph Store\n        :return:\n        \"\"\"\n        self._post_or_put('post', graph_uri, source, format)\n\n    def get(self, graph_uri) -&gt; Graph:\n        \"\"\"\n        Retrieves the data from a graph in the Graph Store\n        :param graph_uri: URI for the graph (if None, the default graph will be used)\n        :return: An [RDFLib Graph][rdflib.Graph]\n        \"\"\"\n        if graph_uri:\n            params = {'graph': str(graph_uri)}\n        else:\n            params = 'default'\n\n        r = requests.get(\n            self._url,\n            params=params,\n            auth=self.auth_details,\n        )\n        r.raise_for_status()\n        g = Graph().parse(r.content)\n        return g\n</code></pre>"},{"location":"reference/ogc/na/gsp/#ogc.na.gsp.GraphStore.__init__","title":"<code>__init__(url, auth_details=None)</code>","text":"<p>Constructs a new GraphStore</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>SPARQL Graph Store Protocol URL</p> required <code>auth_details</code> <code>tuple[str, str] | None</code> <p>tuple in the form ('username', 'password') for authentication</p> <code>None</code> Source code in <code>ogc/na/gsp.py</code> <pre><code>def __init__(self, url: str, auth_details: tuple[str, str] | None = None):\n    \"\"\"\n    Constructs a new GraphStore\n    :param url: SPARQL Graph Store Protocol URL\n    :param auth_details: tuple in the form ('username', 'password') for authentication\n    \"\"\"\n    self._url = url\n    self.auth_details = auth_details\n    self.put = self.replace\n    self.post = self.add\n</code></pre>"},{"location":"reference/ogc/na/gsp/#ogc.na.gsp.GraphStore.add","title":"<code>add(graph_uri, source, format='text/turtle')</code>","text":"<p>Adds data from the provided source to a Graph Store graph (HTTP PUT operation)</p> <p>Parameters:</p> Name Type Description Default <code>graph_uri</code> <code>str | None</code> <p>URI for the graph (if None, the default graph will be used)</p> required <code>source</code> <code>str | bytes | Path</code> <p>Source data or file name</p> required <code>format</code> <code>str</code> <p>Media type to provide to the Graph Store</p> <code>'text/turtle'</code> <p>Returns:</p> Type Description Source code in <code>ogc/na/gsp.py</code> <pre><code>def add(self, graph_uri: str | None, source: str | bytes | Path, format: str = 'text/turtle'):\n    \"\"\"\n    Adds data from the provided source to a Graph Store graph (HTTP PUT operation)\n    :param graph_uri: URI for the graph (if None, the default graph will be used)\n    :param source: Source data or file name\n    :param format: Media type to provide to the Graph Store\n    :return:\n    \"\"\"\n    self._post_or_put('post', graph_uri, source, format)\n</code></pre>"},{"location":"reference/ogc/na/gsp/#ogc.na.gsp.GraphStore.delete","title":"<code>delete(graph_uri, ignore_404=True)</code>","text":"<p>Deletes a graph from the Graph Store</p> <p>Parameters:</p> Name Type Description Default <code>graph_uri</code> <code>str | None</code> <p>URI for the graph (if None, the default graph will be used)</p> required <code>ignore_404</code> <p>Whether to ignore HTTP 404 errors (otherwise, an Exception will be thrown)</p> <code>True</code> <p>Returns:</p> Type Description Source code in <code>ogc/na/gsp.py</code> <pre><code>def delete(self, graph_uri: str | None, ignore_404=True):\n    \"\"\"\n    Deletes a graph from the Graph Store\n    :param graph_uri: URI for the graph (if None, the default graph will be used)\n    :param ignore_404: Whether to ignore HTTP 404 errors (otherwise, an Exception will be thrown)\n    :return:\n    \"\"\"\n    if graph_uri:\n        params = {'graph': str(graph_uri)}\n    else:\n        params = 'default'\n\n    r = requests.delete(\n        self._url,\n        params=params,\n        auth=self.auth_details,\n    )\n    if not (ignore_404 and r.status_code == 404):\n        r.raise_for_status()\n</code></pre>"},{"location":"reference/ogc/na/gsp/#ogc.na.gsp.GraphStore.get","title":"<code>get(graph_uri)</code>","text":"<p>Retrieves the data from a graph in the Graph Store</p> <p>Parameters:</p> Name Type Description Default <code>graph_uri</code> <p>URI for the graph (if None, the default graph will be used)</p> required <p>Returns:</p> Type Description <code>Graph</code> <p>An RDFLib Graph</p> Source code in <code>ogc/na/gsp.py</code> <pre><code>def get(self, graph_uri) -&gt; Graph:\n    \"\"\"\n    Retrieves the data from a graph in the Graph Store\n    :param graph_uri: URI for the graph (if None, the default graph will be used)\n    :return: An [RDFLib Graph][rdflib.Graph]\n    \"\"\"\n    if graph_uri:\n        params = {'graph': str(graph_uri)}\n    else:\n        params = 'default'\n\n    r = requests.get(\n        self._url,\n        params=params,\n        auth=self.auth_details,\n    )\n    r.raise_for_status()\n    g = Graph().parse(r.content)\n    return g\n</code></pre>"},{"location":"reference/ogc/na/gsp/#ogc.na.gsp.GraphStore.replace","title":"<code>replace(graph_uri, source, format='text/turtle')</code>","text":"<p>Replaces the data for a Graph Store graph with the provided source (HTTP PUT operation)</p> <p>Parameters:</p> Name Type Description Default <code>graph_uri</code> <code>str | None</code> <p>URI for the graph (if None, the default graph will be used)</p> required <code>source</code> <code>str | bytes | Path</code> <p>Source data or file name</p> required <code>format</code> <code>str</code> <p>Media type to provide to the Graph Store</p> <code>'text/turtle'</code> <p>Returns:</p> Type Description Source code in <code>ogc/na/gsp.py</code> <pre><code>def replace(self, graph_uri: str | None, source: str | bytes | Path, format: str = 'text/turtle'):\n    \"\"\"\n    Replaces the data for a Graph Store graph with the provided source (HTTP PUT operation)\n    :param graph_uri: URI for the graph (if None, the default graph will be used)\n    :param source: Source data or file name\n    :param format: Media type to provide to the Graph Store\n    :return:\n    \"\"\"\n    self._post_or_put('put', graph_uri, source, format)\n</code></pre>"},{"location":"reference/ogc/na/ingest_json/","title":"ingest_json","text":"<p>This module contains classes to perform JSON-LD uplifting operations, facilitating the conversion of standard JSON into JSON-LD.</p> <p>JSON-LD uplifting is done in 4 steps:</p> <ul> <li>Input filter pre-processing (e.g., csv). This step is optional.</li> <li>Initial transformation using jq expressions (<code>transform</code>).</li> <li>Class annotation (adding <code>@type</code> to the root object and/or to specific nodes, using   jsonpath-ng expressions) (<code>types</code>).</li> <li>Injecting custom JSON-LD <code>@context</code> either globally or inside specific nodes (using   jsonpath-ng expressions (<code>context</code>).</li> </ul> <p>The details for each of these operations are declared inside context definition files, which are YAML documents containing specifications for the uplift workflow. For each input JSON file, its corresponding YAML context definition is detected at runtime:</p> <ol> <li>A domain configuration can be used, which is a JSON (or YAML) document that defines JSON file to context definition mappings.</li> <li>If no registry is used or the input file is not in the registry, a file with the same name but <code>.yml</code> extension will be used, if it exists.</li> <li>Otherwise, a <code>_json-context.yml</code> file in the same directory will be used, if it exists.</li> </ol> <p>If no context definition file is found after performing the previous 3 steps, then the file will be skipped.</p>"},{"location":"reference/ogc/na/ingest_json/#ogc.na.ingest_json.filenames_from_context","title":"<code>filenames_from_context(context_fn, domain_config)</code>","text":"<p>Tries to find a JSON/JSON-LD file from a given YAML context definition filename. Priority:   1. Context file with same name as JSON doc (e.g. test.yml/test.json)   2. Context file in domain configuration (if one provided)   3. Context file in directory (_json-context.yml or _json-context.yaml)</p> <p>Parameters:</p> Name Type Description Default <code>context_fn</code> <code>Path | str</code> <p>YAML context definition filename</p> required <code>domain_config</code> <code>DomainConfiguration | None</code> <p>dict of jsonFile:yamlContextFile mappings</p> required <p>Returns:</p> Type Description <code>list[Path]</code> <p>corresponding JSON/JSON-LD filename, if found</p> Source code in <code>ogc/na/ingest_json.py</code> <pre><code>def filenames_from_context(context_fn: Path | str,\n                           domain_config: DomainConfiguration | None) -&gt; list[Path]:\n    \"\"\"\n    Tries to find a JSON/JSON-LD file from a given YAML context definition filename.\n    Priority:\n      1. Context file with same name as JSON doc (e.g. test.yml/test.json)\n      2. Context file in domain configuration (if one provided)\n      3. Context file in directory (_json-context.yml or _json-context.yaml)\n    :param context_fn: YAML context definition filename\n    :param domain_config: dict of jsonFile:yamlContextFile mappings\n    :return: corresponding JSON/JSON-LD filename, if found\n    \"\"\"\n\n    result = set()\n\n    if not isinstance(context_fn, Path):\n        context_fn = Path(context_fn)\n\n    # 1. Lookup by matching filename\n    if re.match(r'.*\\.json-?(ld)?$', context_fn.stem):\n        # If removing extension results in a JSON/JSON-LD\n        # filename, try it\n        json_fn = context_fn.with_suffix('')\n        if json_fn.is_file():\n            result.add(json_fn)\n    # Otherwise check with appended JSON/JSON-LD extensions\n    for suffix in ('.json', '.jsonld', '.json-ld'):\n        json_fn = context_fn.with_suffix(suffix)\n        if json_fn.is_file():\n            result.add(json_fn)\n\n    # 2. Reverse lookup in registry\n    if domain_config:\n        result.update(domain_config.uplift_entries.find_files_by_context_fn(context_fn))\n\n    # 3. If directory context file, all .json files in directory\n    # NOTE: no .jsonld or .json-ld files, since those could come\n    #   from the output of this very script\n    # NOTE: excluding those files present in the registry\n    if context_fn.stem == '_json-context':\n        with scandir(context_fn.parent) as it:\n            return [x.path for x in cast(it, Iterable)\n                    if x.is_file() and x.name.endswith('.json')]\n\n    return list(result)\n</code></pre>"},{"location":"reference/ogc/na/ingest_json/#ogc.na.ingest_json.find_contexts","title":"<code>find_contexts(filename, domain_config=None)</code>","text":"<p>Find the YAML context file for a given filename, with the following precedence:     1. Search in registry (if provided)     2. Search file with same base name but with yaml/yml extension.     3. Find _json-context.yml/yaml file in same directory</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>Path | str</code> <p>the filename for which to find the context</p> required <code>domain_config</code> <code>DomainConfiguration | None</code> <p>an optional filename:yamlContextFile mapping</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Path | str] | None</code> <p>the YAML context definition paths (Path) and/or profile URIs (str)</p> Source code in <code>ogc/na/ingest_json.py</code> <pre><code>def find_contexts(filename: Path | str,\n                  domain_config: DomainConfiguration | None = None) -&gt; list[Path | str] | None:\n    \"\"\"\n    Find the YAML context file for a given filename, with the following precedence:\n        1. Search in registry (if provided)\n        2. Search file with same base name but with yaml/yml extension.\n        3. Find _json-context.yml/yaml file in same directory\n    :param filename: the filename for which to find the context\n    :param domain_config: an optional filename:yamlContextFile mapping\n    :return: the YAML context definition paths (Path) and/or profile URIs (str)\n    \"\"\"\n\n    if not isinstance(filename, Path):\n        filename = Path(filename)\n\n    # 1. Registry lookup\n    if domain_config:\n        entry: UpliftConfigurationEntry = domain_config.uplift_entries.find_entry_for_file(filename)\n        if entry:\n            return entry.uplift_definitions\n\n    # 2. Same filename with yml/yaml extension or autodetect in dir\n    for context_path in (\n        filename.with_suffix('.yml'),\n        filename.with_suffix('.yaml'),\n        filename.with_suffix('').with_suffix('.yml'),\n        filename.with_suffix('').with_suffix('.yaml'),\n        filename.parent / '_json-context.yml',\n        filename.parent / '_json-context.yaml',\n    ):\n        if context_path.is_file() and not (filename.suffix == '.jsonld' and filename.with_suffix('.json').is_file()):\n            logger.info(f'Autodetected context {context_path} for file {filename}')\n            return [context_path]\n</code></pre>"},{"location":"reference/ogc/na/ingest_json/#ogc.na.ingest_json.generate_graph","title":"<code>generate_graph(input_data, context=None, base=None, fetch_url_whitelist=None, transform_args=None)</code>","text":"<p>Create a graph from an input JSON document and a YAML context definition file.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict | list</code> <p>input JSON data in dict or list format</p> required <code>context</code> <code>dict[str, Any] | Sequence[dict]</code> <p>context definition in dict format, or list thereof</p> <code>None</code> <code>base</code> <code>str | None</code> <p>base URI for JSON-LD context</p> <code>None</code> <code>fetch_url_whitelist</code> <code>Sequence[str] | bool | None</code> <p>list of regular expressions to filter referenced JSON-LD context URLs before retrieving them. If None, it will not be used; if empty sequence or False, remote fetching operations will throw an exception.</p> <code>None</code> <code>transform_args</code> <code>dict | None</code> <p>Additional arguments to pass as variables to the jq transform</p> <code>None</code> <p>Returns:</p> Type Description <code>UpliftResult</code> <p>a tuple with the resulting RDFLib Graph and the JSON-LD enriched file name</p> Source code in <code>ogc/na/ingest_json.py</code> <pre><code>def generate_graph(input_data: dict | list,\n                   context: dict[str, Any] | Sequence[dict] = None,\n                   base: str | None = None,\n                   fetch_url_whitelist: Sequence[str] | bool | None = None,\n                   transform_args: dict | None = None) -&gt; UpliftResult:\n    \"\"\"\n    Create a graph from an input JSON document and a YAML context definition file.\n\n    :param input_data: input JSON data in dict or list format\n    :param context: context definition in dict format, or list thereof\n    :param base: base URI for JSON-LD context\n    :param fetch_url_whitelist: list of regular expressions to filter referenced JSON-LD context URLs before\n        retrieving them. If None, it will not be used; if empty sequence or False, remote fetching operations will\n        throw an exception.\n    :param transform_args: Additional arguments to pass as variables to the jq transform\n    :return: a tuple with the resulting RDFLib Graph and the JSON-LD enriched file name\n    \"\"\"\n\n    if not isinstance(input_data, dict) and not isinstance(input_data, list):\n        raise ValueError('input_data must be a list or dictionary')\n\n    g = Graph()\n    jdoc_ld = input_data\n    if context:\n        base_uri = None\n        for prefix, ns in DEFAULT_NAMESPACES.items():\n            g.bind(prefix, Namespace(ns))\n\n        context_list = context if isinstance(context, Sequence) else (context,)\n        for context_entry in context_list:\n            base_uri = context_entry.get('base-uri', base_uri)\n            jdoc_ld = uplift_json(input_data, context_entry,\n                                  transform_args=transform_args)\n            if 'context' in context_entry:\n                if '$' in context_entry['context']:\n                    root_ctx = context_entry['context']['$']\n                elif '.' in context_entry['context']:\n                    root_ctx = context_entry['context']['.']\n                else:\n                    continue\n\n                if isinstance(root_ctx, dict):\n                    for term, term_val in root_ctx.items():\n                        if not term.startswith('@') \\\n                                and isinstance(term_val, str) \\\n                                and re.match(r'.+[#/:]$', term_val) \\\n                                and is_iri(term_val):\n                            g.bind(term, term_val)\n\n        if not base:\n            if base_uri:\n                base = context['base-uri']\n            elif '@context' in jdoc_ld:\n                # Try to extract from @context\n                # If it is a list, iterate until @base is found\n                base = None\n                if isinstance(jdoc_ld['@context'], list):\n                    for entry in jdoc_ld['@context']:\n                        if not isinstance(entry, dict):\n                            continue\n                        base = entry.get('@base')\n                        if base:\n                            break\n                else:\n                    # If not a list, just look @base up\n                    base = jdoc_ld['@context'].get('@base')\n        if logger.isEnabledFor(logging.DEBUG):\n            logger.debug('Uplifted JSON:\\n%s', json.dumps(jdoc_ld, indent=2))\n\n    def remote_context_url_filter(url_whitelist: str | list[str], url: str):\n        if url_whitelist is False:\n            return False\n        if url_whitelist is True or url_whitelist is None:\n            return True\n        if isinstance(url_whitelist, str):\n            url_whitelist = re.compile(url_whitelist)\n        else:\n            url_whitelist = [re.compile(x) for x in url_whitelist if x]\n        return any(re.match(r, url) for r in url_whitelist)\n\n    g.parse(data=json.dumps(jdoc_ld), format='json-ld', base=base,\n            remote_context_url_filter=functools.partial(remote_context_url_filter, fetch_url_whitelist))\n\n    return UpliftResult(graph=g, uplifted_json=jdoc_ld)\n</code></pre>"},{"location":"reference/ogc/na/ingest_json/#ogc.na.ingest_json.process","title":"<code>process(input_files, domain_cfg=None, context_fn=None, jsonld_fn=False, ttl_fn=False, batch=False, base=None, skip_on_missing_context=False, provenance_base_uri=None, fetch_url_whitelist=None, transform_args=None, file_filter=None)</code>","text":"<p>Performs the JSON-LD uplift process.</p> <p>Parameters:</p> Name Type Description Default <code>input_files</code> <code>str | Path | Sequence[str | Path]</code> <p>list of input, plain JSON files</p> required <code>domain_cfg</code> <code>DomainConfiguration | None</code> <p>domain configuration including uplift definition locations</p> <code>None</code> <code>context_fn</code> <code>str | Path | None</code> <p>used to force the YAML context file name for the uplift. If <code>None</code>, it will be autodetected</p> <code>None</code> <code>jsonld_fn</code> <code>bool | str | Path | None</code> <p>output file name for the JSON-LD content. If it is <code>False</code>, no JSON-LD output will be generated. If it is <code>None</code>, output will be written to stdout.</p> <code>False</code> <code>ttl_fn</code> <code>bool | str | Path | None</code> <p>output file name for the Turtle RDF content. If it is <code>False</code>, no Turtle output will be generated. If it is <code>None</code>, output will be written to stdout.</p> <code>False</code> <code>batch</code> <code>bool</code> <p>in batch mode, all JSON input files are obtained from the context registry and processed</p> <code>False</code> <code>base</code> <code>str</code> <p>base URI to employ</p> <code>None</code> <code>skip_on_missing_context</code> <code>bool</code> <p>whether to silently fail if no context file is found</p> <code>False</code> <code>provenance_base_uri</code> <code>Optional[Union[str, bool]]</code> <p>base URI for provenance resources</p> <code>None</code> <code>fetch_url_whitelist</code> <code>Optional[Union[Sequence, bool]]</code> <p>list of regular expressions to filter referenced JSON-LD context URLs before retrieving them. If None, it will not be used; if empty sequence or False, remote fetching operations will throw an exception</p> <code>None</code> <code>transform_args</code> <code>dict | None</code> <p>Additional arguments to pass as variables to the jq transform</p> <code>None</code> <code>file_filter</code> <code>str | Pattern</code> <p>Filename filter for input files</p> <code>None</code> <p>Returns:</p> Type Description <code>list[UpliftResult]</code> <p>a list of JSON-LD and/or Turtle output files</p> Source code in <code>ogc/na/ingest_json.py</code> <pre><code>def process(input_files: str | Path | Sequence[str | Path],\n            domain_cfg: DomainConfiguration | None = None,\n            context_fn: str | Path | None = None,\n            jsonld_fn: bool | str | Path | None = False,\n            ttl_fn: bool | str | Path | None = False,\n            batch: bool = False,\n            base: str = None,\n            skip_on_missing_context: bool = False,\n            provenance_base_uri: Optional[Union[str, bool]] = None,\n            fetch_url_whitelist: Optional[Union[Sequence, bool]] = None,\n            transform_args: dict | None = None,\n            file_filter: str | re.Pattern = None) -&gt; list[UpliftResult]:\n    \"\"\"\n    Performs the JSON-LD uplift process.\n\n    :param input_files: list of input, plain JSON files\n    :param domain_cfg: domain configuration including uplift definition locations\n    :param context_fn: used to force the YAML context file name for the uplift. If `None`,\n           it will be autodetected\n    :param jsonld_fn: output file name for the JSON-LD content. If it is `False`, no JSON-LD\n           output will be generated. If it is `None`, output will be written to stdout.\n    :param ttl_fn: output file name for the Turtle RDF content. If it is `False`, no Turtle\n           output will be generated. If it is `None`, output will be written to stdout.\n    :param batch: in batch mode, all JSON input files are obtained from the context registry\n           and processed\n    :param base: base URI to employ\n    :param skip_on_missing_context: whether to silently fail if no context file is found\n    :param provenance_base_uri: base URI for provenance resources\n    :param fetch_url_whitelist: list of regular expressions to filter referenced JSON-LD context URLs before\n        retrieving them. If None, it will not be used; if empty sequence or False, remote fetching operations will\n        throw an exception\n    :param transform_args: Additional arguments to pass as variables to the jq transform\n    :param file_filter: Filename filter for input files\n    :return: a list of JSON-LD and/or Turtle output files\n    \"\"\"\n    result: list[UpliftResult] = []\n    process_id = str(uuid.uuid4())\n    workdir = Path()\n    if isinstance(input_files, str) or not isinstance(input_files, Sequence):\n        input_files = (input_files,)\n    if batch:\n        logger.info(\"Input files: %s\", input_files)\n        remaining_fn: deque = deque()\n        for input_file in input_files:\n            if isinstance(input_file, str):\n                for x in filter(lambda x: x, input_file.split(',')):\n                    if '?' in x or '#' in x:\n                        remaining_fn.extend(workdir.glob(x))\n                    else:\n                        remaining_fn.append(x)\n            else:\n                remaining_fn.append(input_file)\n        while remaining_fn:\n            fn = str(remaining_fn.popleft())\n\n            if not fn or not os.path.isfile(fn):\n                continue\n\n            if file_filter and not re.search(file_filter, fn):\n                continue\n\n            if re.match(r'.*\\.ya?ml$', fn):\n                # Context file found, try to find corresponding JSON/JSON-LD file(s)\n                logger.info('Potential YAML context file found: %s', fn)\n                remaining_fn.extend(filenames_from_context(fn, domain_config=domain_cfg) or [])\n                continue\n\n            logger.info('File %s matches, processing', fn)\n            try:\n                result.append(process_file(\n                    fn,\n                    jsonld_fn=False if jsonld_fn is False else None,\n                    ttl_fn=False if ttl_fn is False else None,\n                    context_fn=None,\n                    base=base,\n                    provenance_base_uri=provenance_base_uri,\n                    provenance_process_id=process_id,\n                    fetch_url_whitelist=fetch_url_whitelist,\n                    domain_cfg=domain_cfg,\n                    transform_args=transform_args,\n                ))\n            except MissingContextException as e:\n                if skip_on_missing_context or batch:\n                    logger.warning(\"Error processing JSON/JSON-LD file, skipping: %s\", getattr(e, 'msg', str(e)))\n                else:\n                    raise\n    else:\n        for input_file in input_files:\n            try:\n                result.append(process_file(\n                    input_file,\n                    jsonld_fn=jsonld_fn if jsonld_fn is not None else '-',\n                    ttl_fn=ttl_fn if ttl_fn is not None else '-',\n                    context_fn=context_fn,\n                    base=base,\n                    provenance_base_uri=provenance_base_uri,\n                    provenance_process_id=process_id,\n                    fetch_url_whitelist=fetch_url_whitelist,\n                    domain_cfg=domain_cfg,\n                    transform_args=transform_args,\n                ))\n            except Exception as e:\n                if skip_on_missing_context:\n                    logger.warning(\"Error processing JSON/JSON-LD file, skipping: %s\", getattr(e, 'msg', str(e)))\n                else:\n                    raise\n\n    return result\n</code></pre>"},{"location":"reference/ogc/na/ingest_json/#ogc.na.ingest_json.process_file","title":"<code>process_file(input_fn, jsonld_fn=False, ttl_fn=False, context_fn=None, domain_cfg=None, base=None, provenance_base_uri=None, provenance_process_id=None, fetch_url_whitelist=None, transform_args=None)</code>","text":"<p>Process input file and generate output RDF files.</p> <p>Parameters:</p> Name Type Description Default <code>input_fn</code> <code>str | Path</code> <p>input filename</p> required <code>jsonld_fn</code> <code>str | Path | bool | None</code> <p>output JSON-lD filename (None for automatic). If False, no JSON-LD output will be generated</p> <code>False</code> <code>ttl_fn</code> <code>str | Path | bool | None</code> <p>output Turtle filename (None for automatic). If False, no Turtle output will be generated.</p> <code>False</code> <code>context_fn</code> <code>str | Path | Sequence[str | Path] | None</code> <p>YAML context filename. If None, will be autodetected: 1. From a file with the same name but yml/yaml extension (test.json -&gt; test.yml) 2. From the domain_cfg 3. From a _json-context.yml/_json-context.yaml file in the same directory</p> <code>None</code> <code>domain_cfg</code> <code>DomainConfiguration | None</code> <p>domain configuration with uplift definition locations</p> <code>None</code> <code>base</code> <code>str | None</code> <p>base URI for JSON-LD</p> <code>None</code> <code>provenance_base_uri</code> <code>str | bool | None</code> <p>base URI for provenance resources</p> <code>None</code> <code>provenance_process_id</code> <code>str | None</code> <p>process identifier for provenance tracking</p> <code>None</code> <code>fetch_url_whitelist</code> <code>bool | Sequence[str] | None</code> <p>list of regular expressions to filter referenced JSON-LD context URLs before retrieving them. If None, it will not be used; if empty sequence or False, remote fetching operations will throw an exception</p> <code>None</code> <code>transform_args</code> <code>dict | None</code> <p>Additional arguments to pass as variables to the jq transform</p> <code>None</code> <p>Returns:</p> Type Description <code>UpliftResult | None</code> <p>List of output files created</p> Source code in <code>ogc/na/ingest_json.py</code> <pre><code>def process_file(input_fn: str | Path,\n                 jsonld_fn: str | Path | bool | None = False,\n                 ttl_fn: str | Path | bool | None = False,\n                 context_fn: str | Path | Sequence[str | Path] | None = None,\n                 domain_cfg: DomainConfiguration | None = None,\n                 base: str | None = None,\n                 provenance_base_uri: str | bool | None = None,\n                 provenance_process_id: str | None = None,\n                 fetch_url_whitelist: bool | Sequence[str] | None = None,\n                 transform_args: dict | None = None) -&gt; UpliftResult | None:\n    \"\"\"\n    Process input file and generate output RDF files.\n\n    :param input_fn: input filename\n    :param jsonld_fn: output JSON-lD filename (None for automatic).\n        If False, no JSON-LD output will be generated\n    :param ttl_fn: output Turtle filename (None for automatic).\n        If False, no Turtle output will be generated.\n    :param context_fn: YAML context filename. If None, will be autodetected:\n        1. From a file with the same name but yml/yaml extension (test.json -&gt; test.yml)\n        2. From the domain_cfg\n        3. From a _json-context.yml/_json-context.yaml file in the same directory\n    :param domain_cfg: domain configuration with uplift definition locations\n    :param base: base URI for JSON-LD\n    :param provenance_base_uri: base URI for provenance resources\n    :param provenance_process_id: process identifier for provenance tracking\n    :param fetch_url_whitelist: list of regular expressions to filter referenced JSON-LD context URLs before\n        retrieving them. If None, it will not be used; if empty sequence or False, remote fetching operations will\n        throw an exception\n    :param transform_args: Additional arguments to pass as variables to the jq transform\n    :return: List of output files created\n    \"\"\"\n\n    start_time = datetime.now()\n\n    if not isinstance(input_fn, Path):\n        input_fn = Path(input_fn)\n\n    if not input_fn.is_file():\n        raise IOError(f'Input is not a file ({input_fn})')\n\n    contexts = []\n    provenance_contexts = []\n    if not context_fn:\n        for found_context in (find_contexts(input_fn, domain_config=domain_cfg) or ()):\n            if isinstance(found_context, Path):\n                contexts.append(util.load_yaml(filename=found_context))\n            else:\n                # Profile URI\n                artifact_urls = domain_cfg.profile_registry.get_artifacts(found_context, profile.ROLE_SEMANTIC_UPLIFT)\n                if artifact_urls:\n                    for a in artifact_urls:\n                        contexts.append(util.load_yaml(a))\n                        provenance_contexts.append(a)\n\n    elif not isinstance(context_fn, Sequence) or isinstance(context_fn, str):\n        provenance_contexts = (context_fn,)\n        contexts = (util.load_yaml(context_fn),)\n    else:\n        provenance_contexts = context_fn\n        contexts = [util.load_yaml(fn) for fn in context_fn]\n\n    if not contexts:\n        raise MissingContextException('No context file provided and one could not be discovered automatically')\n\n    # Apply input filter of first context only (if any)\n    input_filters = contexts[0].get('input-filter')\n    if input_filters:\n        if not isinstance(input_filters, dict):\n            raise ValueError('input-filter must be an object')\n        input_data = apply_input_filter(input_fn, input_filters)\n    else:\n        with open(input_fn, 'r') as j:\n            input_data = json.load(j)\n\n    if logger.isEnabledFor(logging.DEBUG):\n        logger.debug('Input data:\\n%s', json.dumps(input_data, indent=2))\n\n    provenance_metadata: ProvenanceMetadata | None = None\n    if provenance_base_uri is not False:\n        used = [FileProvenanceMetadata(filename=input_fn, mime_type='application/json')]\n        used.extend(FileProvenanceMetadata(filename=c, mime_type='application/yaml') for c in provenance_contexts)\n        provenance_metadata = ProvenanceMetadata(\n            used=used,\n            batch_activity_id=provenance_process_id,\n            base_uri=provenance_base_uri,\n            root_directory=os.getcwd(),\n            start=start_time,\n            end_auto=True,\n        )\n\n    if transform_args is None:\n        transform_args = {}\n    transform_args['_filename'] = str(input_fn.resolve())\n    transform_args['_basename'] = str(input_fn.name)\n    transform_args['_dirname'] = str(input_fn.resolve().parent)\n    transform_args['_relname'] = os.path.relpath(input_fn)\n\n    if not base:\n        base = str(input_fn)\n\n    uplift_result = generate_graph(input_data,\n                                   context=contexts,\n                                   base=base,\n                                   fetch_url_whitelist=fetch_url_whitelist,\n                                   transform_args=transform_args)\n\n    uplift_result.input_file = input_fn\n\n    # False = do not generate\n    # None = auto filename\n    # - = stdout\n    if ttl_fn is not False:\n        if ttl_fn == '-':\n            if provenance_metadata:\n                provenance_metadata.output = FileProvenanceMetadata(mime_type='text/turtle', use_bnode=False)\n                generate_provenance(uplift_result.graph, provenance_metadata)\n            print(uplift_result.graph.serialize(format='ttl'))\n        else:\n            if not ttl_fn:\n                ttl_fn = input_fn.with_suffix('.ttl') \\\n                    if input_fn.suffix != '.ttl' \\\n                    else input_fn.with_suffix(input_fn.suffix + '.ttl')\n            if provenance_metadata:\n                provenance_metadata.output = FileProvenanceMetadata(filename=ttl_fn,\n                                                                    mime_type='text/turtle',\n                                                                    use_bnode=False)\n                generate_provenance(uplift_result.graph, provenance_metadata)\n            uplift_result.graph.serialize(destination=ttl_fn, format='ttl')\n            uplift_result.output_files.append(ttl_fn)\n\n    # False = do not generate\n    # None = auto filename\n    # \"-\" = stdout\n    if jsonld_fn is not False:\n        if jsonld_fn == '-':\n            print(json.dumps(uplift_result.uplifted_json, indent=2))\n        else:\n            if not jsonld_fn:\n                jsonld_fn = input_fn.with_suffix('.jsonld') \\\n                    if input_fn.suffix != '.jsonld' \\\n                    else input_fn.with_suffix(input_fn.suffix + '.jsonld')\n\n            with open(jsonld_fn, 'w') as f:\n                json.dump(uplift_result.uplifted_json, f, indent=2)\n            uplift_result.output_files.append(jsonld_fn)\n\n    return uplift_result\n</code></pre>"},{"location":"reference/ogc/na/ingest_json/#ogc.na.ingest_json.uplift_json","title":"<code>uplift_json(data, context, fetch_url_whitelist=None, transform_args=None)</code>","text":"<p>Transform a JSON document loaded in a dict, and embed JSON-LD context into it.</p> <p>WARNING: This function modifies the input dict. If that is not desired, make a copy before invoking.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict | list</code> <p>the JSON document in dict format</p> required <code>context</code> <code>dict</code> <p>YAML context definition</p> required <code>fetch_url_whitelist</code> <code>Optional[Union[Sequence, bool]]</code> <p>list of regular expressions to filter referenced JSON-LD context URLs before retrieving them. If None, it will not be used; if empty sequence or False, remote fetching operations will throw an exception.</p> <code>None</code> <code>transform_args</code> <code>dict | None</code> <p>Additional arguments to pass as variables to the jq transform</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>the transformed and JSON-LD-enriched data</p> Source code in <code>ogc/na/ingest_json.py</code> <pre><code>def uplift_json(data: dict | list, context: dict,\n                fetch_url_whitelist: Optional[Union[Sequence, bool]] = None,\n                transform_args: dict | None = None) -&gt; dict:\n    \"\"\"\n    Transform a JSON document loaded in a dict, and embed JSON-LD context into it.\n\n    WARNING: This function modifies the input dict. If that is not desired, make a copy\n    before invoking.\n\n    :param data: the JSON document in dict format\n    :param context: YAML context definition\n    :param fetch_url_whitelist: list of regular expressions to filter referenced JSON-LD context URLs before\n        retrieving them. If None, it will not be used; if empty sequence or False, remote fetching operations will\n        throw an exception.\n    :param transform_args: Additional arguments to pass as variables to the jq transform\n    :return: the transformed and JSON-LD-enriched data\n    \"\"\"\n\n    context_position = context.get('position', 'before')\n\n    validate_context(context, transform_args=transform_args)\n\n    # Check whether @graph scoping is necessary for transformations and paths\n    scoped_graph = context.get('scope', 'graph') == 'graph' and '@graph' in data\n    data_graph = data['@graph'] if scoped_graph else data\n\n    # Check if pre-transform necessary\n    transform = context.get('transform')\n    if transform:\n        # Allow for transform lists to do sequential transformations\n        if isinstance(transform, str):\n            transform = (transform,)\n        for i, t in enumerate(transform):\n            tranformed_txt = jq.compile(t, args=transform_args).input(data_graph).text()\n            if logger.isEnabledFor(logging.DEBUG):\n                logger.debug('After transform %d:\\n%s', i + 1, tranformed_txt)\n            data_graph = json.loads(tranformed_txt)\n\n    # Add types\n    types = context.get('types', {})\n    for loc, type_list in types.items():\n        items = json_path_parse(loc).find(data_graph)\n        if isinstance(type_list, str):\n            type_list = [type_list]\n        for item in items:\n            existing = item.value.setdefault('@type', [])\n            if isinstance(existing, str):\n                item.value['@type'] = [existing] + type_list\n            else:\n                item.value['@type'].extend(type_list)\n            item_types = item.value.get('@type')\n            if not item_types:\n                item.value.pop('@type', None)\n            elif isinstance(item_types, Sequence) and not isinstance(item_types, str) and len(item_types) == 1:\n                item.value['@type'] = item_types[0]\n\n    # Add contexts\n    context_list = context.get('context', {})\n    global_context = None\n    for loc, val in context_list.items():\n        if not loc or loc in ['.', '$']:\n            global_context = val\n        else:\n            items = json_path_parse(loc).find(data_graph)\n            for item in items:\n                item.value['@context'] = _get_injected_context(item.value, val, context_position)\n\n    if isinstance(data_graph, dict):\n        data_context = data_graph.pop('@context', None)\n        if data_context:\n            if not global_context:\n                global_context = data_context\n            elif isinstance(global_context, list):\n                global_context.extend(data_context)\n            else:\n                global_context = [data_context, global_context]\n\n    if (global_context and not isinstance(data_graph, dict)) or scoped_graph:\n        return {\n            '@context': _get_injected_context(data, global_context, context_position),\n            '@graph': data_graph,\n        }\n    else:\n        if global_context:\n            return {\n                '@context': _get_injected_context(data, global_context, context_position),\n                **data_graph\n            }\n        return data_graph\n</code></pre>"},{"location":"reference/ogc/na/models/","title":"models","text":""},{"location":"reference/ogc/na/models/#ogc.na.models.ValidationReport","title":"<code>ValidationReport</code>","text":"<p>Validation report from a single validation result.</p> <p>Structures a pySHACL tuple report into <code>result</code>, <code>graph</code> and <code>text</code> fields.</p> Source code in <code>ogc/na/models.py</code> <pre><code>class ValidationReport:\n    \"\"\"\n    Validation report from a single validation result.\n\n    Structures a pySHACL tuple report into `result`, `graph` and `text`\n    fields.\n    \"\"\"\n\n    def __init__(self, pyshacl_result: tuple,\n                 used_resources: Iterable[Union[str, Path]] = None):\n        \"\"\"\n        :param pyshacl_result: result from executing [pyshacl.validate]\n        \"\"\"\n        self.result, self.graph, self.text = pyshacl_result\n        self.used_resources: set[Union[str, Path]] = set(used_resources) if used_resources else set()\n</code></pre>"},{"location":"reference/ogc/na/models/#ogc.na.models.ValidationReport.__init__","title":"<code>__init__(pyshacl_result, used_resources=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pyshacl_result</code> <code>tuple</code> <p>result from executing [pyshacl.validate]</p> required Source code in <code>ogc/na/models.py</code> <pre><code>def __init__(self, pyshacl_result: tuple,\n             used_resources: Iterable[Union[str, Path]] = None):\n    \"\"\"\n    :param pyshacl_result: result from executing [pyshacl.validate]\n    \"\"\"\n    self.result, self.graph, self.text = pyshacl_result\n    self.used_resources: set[Union[str, Path]] = set(used_resources) if used_resources else set()\n</code></pre>"},{"location":"reference/ogc/na/profile/","title":"profile","text":"<p>This is a support module for parsing profile metadata and applying entailment, validation and annotation operations to RDF graphs.</p> <p>Conformance to a given profile is declared by using <code>dcterms:conformsTo</code>.</p> <p>This module uses the following resource roles (where the <code>profrole</code> prefix is http://www.w3.org/ns/dx/prof/role/):</p> <ul> <li><code>profrole:entailment</code> for entailment operations (needs to conform to SHACL).</li> <li><code>profrole:entailment-closure</code> is used as extra ontological information for entailment.</li> <li><code>profrole:validation</code> for validation operations (needs to conform to SHACL).</li> <li><code>profrole:validation-closure</code> is used as extra ontological information for validation.</li> <li><code>profrole:annotation</code> is loaded as additional ontological annotation data.</li> </ul>"},{"location":"reference/ogc/na/profile/#ogc.na.profile.ProfileRegistry","title":"<code>ProfileRegistry</code>","text":"Source code in <code>ogc/na/profile.py</code> <pre><code>class ProfileRegistry:\n\n    def __init__(self, srcs: str | Path | Iterable[str | Path],\n                 local_artifact_mappings: dict[str, str | Path] = None,\n                 ignore_artifact_errors=False):\n\n        assert srcs is not None\n        if isinstance(srcs, str) or not isinstance(srcs, Iterable):\n            self._srcs = (srcs,)\n        else:\n            self._srcs = srcs\n\n        self._local_artifact_mappings: dict[str, Union[str, Path]] = {}\n        if local_artifact_mappings:\n            self._local_artifact_mappings = {u: Path(p) for u, p in local_artifact_mappings.items()}\n        logger.debug(\"Using local artifact mappings: %s\", self._local_artifact_mappings)\n        self.profiles: dict[URIRef, Profile] = {}\n        self._load_profiles()\n        # Cache of { profile: { role: Graph } }\n        self._graphs: dict[URIRef, dict[URIRef,\n                           tuple[Graph, set[Union[str, Path]], set[Union[str, Path]]]]] = {}\n\n        self.ignore_artifact_errors = ignore_artifact_errors\n\n    def build_profile_chain(self, profiles: Sequence[URIRef | str],\n                            recursive: bool = True,\n                            sort: bool = True) -&gt; list[URIRef]:\n        if not profiles:\n            return []\n\n        if not sort:\n            # Only known profiles and remove duplicates\n            known = {URIRef(p): True for p in profiles if p in self.profiles}\n            if recursive:\n                pending = set(known)\n                while pending:\n                    prof = self.profiles.get(pending.pop())\n                    if prof:\n                        for prof_of in prof.profile_of or ():\n                            if prof_of not in known:\n                                pending.add(prof_of)\n                            known[prof_of] = True\n            return list(known)\n\n        # Otherwise, sort DAG\n        # 1. Build dependency tree\n        dependencies: dict[URIRef, set[URIRef] | None] = {}\n        pending = deque(profiles)  # using a deque to try and preserve as much of the original order as possible\n        while pending:\n            prof_uri = pending.popleft()\n            if prof_uri in dependencies:\n                # skip if already processed\n                continue\n            prof = self.profiles.get(prof_uri)\n            if not prof:\n                # skip if unknown\n                continue\n            prof_deps = [d for d in self.profiles.get(prof_uri).profile_of if d in self.profiles]\n            if not prof_deps:\n                # has no dependencies\n                dependencies[prof_uri] = None\n            if not recursive:\n                # non-recursive =&gt; only work with provided profile list\n                prof_deps = [d for d in prof_deps if d in profiles]\n            else:\n                for p in prof_deps:\n                    if p not in pending:\n                        pending.appendleft(p)\n            dependencies[prof_uri] = set(prof_deps)\n\n        result: list[URIRef] = []\n        # 2. Sort dependencies\n        while True:\n            if not dependencies:\n                break\n            removed = {}\n            for parent_uri, child_uris in dependencies.items():\n                if not child_uris:\n                    removed[parent_uri] = True\n            if not removed:\n                dependencies_str = '; '.join(\n                    f\"{p} &lt;- {', '.join(str(c) for c in cs)}\" for p, cs in dependencies.items())\n                raise ValueError(f'Cycle detected in profile DAG, cannot sort: {dependencies_str}')\n            for rem in removed:\n                del dependencies[rem]\n                result.append(rem)\n            removed = set(removed)\n            for child_uris in dependencies.values():\n                child_uris -= removed\n\n        return result\n\n    def _load_profiles(self):\n        logger.debug(\"Loading profiles from %s\", [str(x) for x in self._srcs])\n        g: Graph = Graph()\n        for src in self._srcs:\n            if isinstance(src, str) and src.startswith('sparql:'):\n                endpoint = src[len('sparql:'):]\n                logger.info(\"Fetching profiles from SPARQL endpoint %s\", endpoint)\n                assert util.is_url(endpoint, http_only=True)\n                s = g.query(PROFILES_QUERY.replace('__SERVICE__', f\"SERVICE &lt;{endpoint}&gt;\")).graph\n                util.copy_triples(s, g)\n            else:\n                g.parse(src)\n\n        # resolve recursive isProfileOf and sameAs\n        g = g.query(PROFILES_QUERY.replace('__SERVICE__', '')).graph\n\n        for profile_ref in cast(list[URIRef], g.subjects(RDF.type, PROF.Profile)):\n\n            if profile_ref in self.profiles:\n                # do not parse duplicate profiles\n                continue\n\n            token = str(g.value(profile_ref, PROF.hasToken))\n            label = g.value(profile_ref, RDFS.label)\n            profile_of: list[URIRef] = cast(list[URIRef], list(g.objects(profile_ref, PROF.isProfileOf)))\n\n            profile = Profile(profile_ref, token, profile_of, label=str(label) if label else None)\n\n            for resource_ref in g.objects(profile_ref, PROF.hasResource):\n                role_ref = g.value(resource_ref, PROF.hasRole)\n                if not role_ref:\n                    continue\n                for artifact_ref in g.objects(resource_ref, PROF.hasArtifact):\n                    profile.add_artifact(role_ref, cast(URIRef, artifact_ref))\n\n            self.profiles[profile_ref] = profile\n            for same_as_ref in g.objects(profile_ref, OWL.sameAs):\n                self.profiles[cast(URIRef, same_as_ref)] = profile\n\n        if logger.isEnabledFor(logging.DEBUG):\n            logger.debug(f\"Profiles loaded: %s\", [str(p) for p in self.profiles])\n        else:\n            logger.info(f\"Loaded {len(self.profiles)} profiles\")\n\n    def _apply_mappings(self, uri: str) -&gt; Path | str:\n        \"\"\"\n        Returns the longest match in self._local_artifact_mappings (prefixes)\n        for a given URI, or the URI itself if not found\n        \"\"\"\n\n        if uri in self._local_artifact_mappings:\n            return self._local_artifact_mappings[uri]\n\n        matchedlocal = None\n        matchedpath = uri\n        for l, p in self._local_artifact_mappings.items():\n            if uri.startswith(l) and (matchedlocal is None or len(matchedlocal) &lt; len(l)):\n                matchedlocal, matchedpath = l, p / uri[len(l):]\n        return matchedpath\n\n    def get_artifacts(self, profile: URIRef | str, role: URIRef) -&gt; set[str | Path] | None:\n        if not isinstance(profile, URIRef):\n            profile = URIRef(profile)\n        if profile not in self.profiles:\n            return None\n\n        return set(self._apply_mappings(artifact_ref) for artifact_ref in self.profiles[profile].get_artifacts(role))\n\n    def get_graph(self, profile: URIRef | str, role: URIRef) \\\n            -&gt; tuple[Graph | None, set[str | Path] | None, set[str | Path] | None]:\n        if not isinstance(profile, URIRef):\n            profile = URIRef(profile)\n        if profile not in self.profiles:\n            return None, None, None\n\n        prof_graphs = self._graphs.setdefault(profile, {})\n\n        if role in prof_graphs:\n            return prof_graphs[role]\n\n        g = Graph()\n        artifacts = set()\n        failed_artifacts = set()\n        for artifact in self.get_artifacts(profile, role):\n            try:\n                g.parse(artifact)\n                artifacts.add(artifact)\n            except Exception as e:\n                if self.ignore_artifact_errors:\n                    logger.warning(\"Error when retrieving or parsing artifact %s: %s\",\n                                   artifact, str(e))\n                    failed_artifacts.add(artifact)\n                else:\n                    raise Exception(f\"Error when retrieving or parsing artifact {artifact}\") from e\n\n            prof_graphs[role] = g, artifacts, failed_artifacts\n        return g, artifacts, failed_artifacts\n\n    def entail(self, g: Graph,\n               additional_profiles: Optional[Sequence[str | URIRef]] = None,\n               inplace: bool = True,\n               recursive: bool = True) -&gt; tuple[Graph, set[Union[str, Path]]]:\n        if not inplace:\n            g = util.copy_triples(g)\n\n        profiles = deque(find_profiles(g))\n        if additional_profiles:\n            profiles.extend(p if isinstance(p, URIRef) else URIRef(p) for p in additional_profiles)\n\n        profiles = self.build_profile_chain(profiles, recursive=recursive)\n\n        artifacts = set()\n        for profile_ref in profiles:\n            logger.info('Entailing with %s', profile_ref)\n            rules, rules_artifacts, failed_artifacts = self.get_graph(profile_ref, ROLE_ENTAILMENT)\n            extra, extra_artifacts, failed_artifacts = self.get_graph(profile_ref, ROLE_ENTAILMENT_CLOSURE)\n            if rules_artifacts:\n                artifacts.update(rules_artifacts)\n            if extra_artifacts:\n                artifacts.update(extra_artifacts)\n            g = util.entail(g, rules, extra or None, True)\n\n        return g, artifacts\n\n    def validate(self, g: Graph,\n                 additional_profiles: Sequence[str | URIRef] | None = None,\n                 recursive: bool = True, log_artifact_errors: bool = False) -&gt; ProfilesValidationReport:\n        result = ProfilesValidationReport()\n        profiles = deque(find_profiles(g))\n        if additional_profiles:\n            profiles.extend(p if isinstance(p, URIRef) else URIRef(p) for p in additional_profiles)\n\n        profiles = self.build_profile_chain(profiles, recursive=recursive, sort=False)\n\n        for profile_ref in profiles:\n            logger.info(\"Validating with %s\", str(profile_ref))\n            profile = self.profiles.get(profile_ref)\n            if not profile:\n                logger.warning(\"Profile %s not found\", profile_ref)\n                # should we fail?\n                continue\n            rules, rules_artifacts, failed_rules_artifacts = self.get_graph(profile_ref, ROLE_VALIDATION)\n            extra, extra_artifacts, failed_extra_artifacts = self.get_graph(profile_ref, ROLE_VALIDATION_CLOSURE)\n            failed_artifacts = failed_rules_artifacts | failed_extra_artifacts\n            try:\n                prof_result = util.validate(g, rules, extra)\n            except Exception as e:\n                if log_artifact_errors:\n                    err_text = f\"Error performing SHACL validation: {e}\\n\"\n                    if rules_artifacts:\n                        err_text += 'Used rules artifacts:\\n' + '\\n - '.join((str(s) for s in rules_artifacts))\n                    if extra_artifacts:\n                        err_text += 'Used extra artifacts:\\n' + '\\n - '.join((str(s) for s in extra_artifacts))\n                    prof_result = ValidationReport((False, Graph(), err_text))\n                else:\n                    all_artifacts = {\n                        'rules': [str(a) for a in rules_artifacts],\n                        'extra': [str(a) for a in extra_artifacts],\n                    }\n                    raise ArtifactError('Error performing SHACL validation', all_artifacts) from e\n\n            prof_result.used_resources = set(itertools.chain(rules_artifacts or [], extra_artifacts or []))\n            if failed_artifacts:\n                prof_result.text += \"\\n# Failed artifacts\\n\" + '\\n'.join(str(a) for a in failed_artifacts) + '\\n'\n            result.add(ProfileValidationReport(profile_ref, profile.token, prof_result))\n            logger.debug(\"Adding validation results for %s\", profile_ref)\n\n        return result\n\n    def get_annotations(self, g: Graph, additional_profiles: Sequence[URIRef | str] | None = None) -&gt; dict[Path, Graph]:\n        result = {}\n        profiles = find_profiles(g)\n        if additional_profiles:\n            profiles = itertools.chain(profiles, additional_profiles)\n        for profile_ref in profiles:\n            artifacts = self.get_artifacts(profile_ref, ROLE_ANNOTATION)\n            for artifact in artifacts:\n                result[artifact] = Graph().parse(artifact)\n        return result\n\n    def has_profile(self, uri: str | URIRef) -&gt; bool:\n        if isinstance(uri, str):\n            uri = URIRef(uri)\n        return uri in self.profiles\n</code></pre>"},{"location":"reference/ogc/na/provenance/","title":"provenance","text":""},{"location":"reference/ogc/na/update_vocabs/","title":"update_vocabs","text":"<p>Implements an entailment + validation workflow.</p> <p><code>update_vocabs</code> starts by loading one or more DomainConfiguration's from RDF files and/or SPARQL endpoints, and a series of profile definitions (also from a list of RDF files and/or SPARQL endpoints). From there, individual or batch processing of files can be done, as well as uploading the results to a target triplestore.</p> <p>This script can be used as a library, or run directly from the cli; please refer to the OGC NamingAuthority repository for usage details on the latter.</p>"},{"location":"reference/ogc/na/update_vocabs/#ogc.na.update_vocabs.get_entailed_base_path","title":"<code>get_entailed_base_path(f, g, rootpattern='/def/', entailed_dir=DEFAULT_ENTAILED_DIR)</code>","text":"<p>Tries to find the base output file path for an entailed version of a source Graph.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Path</code> <p>the original path of the source file</p> required <code>g</code> <code>Graph</code> <p>the Graph loaded from the source file</p> required <code>rootpattern</code> <code>str</code> <p>a root pattern to filter candidate URIs</p> <code>'/def/'</code> <code>entailed_dir</code> <code>str</code> <p>the name of the base entailed files directory</p> <code>DEFAULT_ENTAILED_DIR</code> Source code in <code>ogc/na/update_vocabs.py</code> <pre><code>def get_entailed_base_path(f: Path, g: Graph, rootpattern: str = '/def/',\n                           entailed_dir: str = DEFAULT_ENTAILED_DIR) -&gt; tuple:\n    \"\"\"\n    Tries to find the base output file path for an entailed version of a source Graph.\n\n    :param f: the original path of the source file\n    :param g: the [Graph][rdflib.Graph] loaded from the source file\n    :param rootpattern: a root pattern to filter candidate URIs\n    :param entailed_dir: the name of the base entailed files directory\n    \"\"\"\n\n    if not rootpattern:\n        # just assume filename is going to be fine\n        return (f.parent / entailed_dir / f.name,\n                f.name, next(get_graph_uri_for_vocab(g), None))\n\n    canonical_filename = None\n    conceptscheme = None\n    multiple_cs_warning = True\n    for graphuri in get_graph_uri_for_vocab(g):\n\n        if rootpattern in graphuri:\n            cs_filename = graphuri.rsplit(rootpattern)[1].split('#', 1)[0]\n            conceptscheme = graphuri\n        else:\n            logger.info('File %s: ignoring concept scheme %s not matching domain path %s',\n                        str(f), graphuri, rootpattern)\n            continue\n\n        if canonical_filename and canonical_filename != cs_filename and multiple_cs_warning:\n            multiple_cs_warning = False\n            logger.warning(\"File %s contains multiple concept schemes\", str(f))\n\n        canonical_filename = cs_filename\n\n    if not canonical_filename:\n        logger.warning('File %s contains no concept schemes matching domain path %s; using filename',\n                       str(f), rootpattern)\n        canonical_filename = f.name\n    elif canonical_filename.startswith('/'):\n        canonical_filename = canonical_filename[1:]\n\n    return (f.parent / entailed_dir / Path(canonical_filename),\n            canonical_filename, conceptscheme)\n</code></pre>"},{"location":"reference/ogc/na/update_vocabs/#ogc.na.update_vocabs.get_graph_uri_for_vocab","title":"<code>get_graph_uri_for_vocab(g=None)</code>","text":"<p>Find a target graph URI in a vocabulary Graph.</p> <p>In effect, this function merely looks for SKOS ConceptScheme's.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>Graph</code> <p>the Graph for which to find the target URI</p> <code>None</code> <p>Returns:</p> Type Description <code>Generator[str, None, None]</code> <p>a Node generator</p> Source code in <code>ogc/na/update_vocabs.py</code> <pre><code>def get_graph_uri_for_vocab(g: Graph = None) -&gt; Generator[str, None, None]:\n    \"\"\"\n    Find a target graph URI in a vocabulary [Graph][rdflib.Graph].\n\n    In effect, this function merely looks for\n    [SKOS ConceptScheme's](https://www.w3.org/TR/2008/WD-skos-reference-20080829/skos.html#ConceptScheme).\n\n    :param g: the [Graph][rdflib.Graph] for which to find the target URI\n    :return: a [Node][rdflib.term.Node] generator\n    \"\"\"\n    for s in g.subjects(predicate=RDF.type, object=SKOS.ConceptScheme):\n        yield str(s)\n</code></pre>"},{"location":"reference/ogc/na/update_vocabs/#ogc.na.update_vocabs.load_vocab","title":"<code>load_vocab(vocab, graph_uri, graph_store, auth_details=None)</code>","text":"<p>Loads a vocabulary onto a triplestore using the SPARQL Graph Store protocol.</p> <p>Parameters:</p> Name Type Description Default <code>vocab</code> <code>Union[Graph, str, Path]</code> <p>the file or Graph to load</p> required <code>graph_uri</code> <code>str</code> <p>a target graph URI</p> required <code>graph_store</code> <code>str</code> <p>the target SPARQL Graph Store protocol URL</p> required <code>auth_details</code> <code>tuple[str]</code> <p>a <code>(username, password)</code> tuple for authentication</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>ogc/na/update_vocabs.py</code> <pre><code>def load_vocab(vocab: Union[Graph, str, Path], graph_uri: str,\n               graph_store: str, auth_details: tuple[str] = None) -&gt; None:\n    \"\"\"\n    Loads a vocabulary onto a triplestore using the [SPARQL Graph Store\n    protocol](https://www.w3.org/TR/sparql11-http-rdf-update/).\n\n    :param vocab: the file or Graph to load\n    :param graph_uri: a target graph URI\n    :param graph_store: the target SPARQL Graph Store protocol URL\n    :param auth_details: a `(username, password)` tuple for authentication\n    :return:\n    \"\"\"\n    # PUT is equivalent to DROP GRAPH + INSERT DATA\n    # Graph is automatically created per Graph Store spec\n\n    if isinstance(vocab, Graph):\n        content = vocab.serialize(format='ttl')\n    else:\n        with open(vocab, 'rb') as f:\n            content = f.read()\n\n    r = requests.put(\n        graph_store,\n        params={\n            'graph': graph_uri,\n        },\n        auth=auth_details,\n        headers={\n            'Content-type': 'text/turtle',\n        },\n        data=content\n    )\n    logger.debug('HTTP status code: %d', r.status_code)\n    r.raise_for_status()\n</code></pre>"},{"location":"reference/ogc/na/update_vocabs/#ogc.na.update_vocabs.make_rdf","title":"<code>make_rdf(filename, g, rootpath='/def/', entailment_directory=DEFAULT_ENTAILED_DIR, provenance_metadata=None)</code>","text":"<p>Serializes entailed RDF graphs in several output formats for a given input graph.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>Union[str, Path]</code> <p>the original source filename</p> required <code>g</code> <code>Graph</code> <p>Graph loaded from the source file</p> required <code>rootpath</code> <p>a path to filter concept schemes inside the Graph and infer the main one</p> <code>'/def/'</code> <code>provenance_metadata</code> <code>ProvenanceMetadata</code> <p>provenance metadata (None to ignore)</p> <code>None</code> <code>entailment_directory</code> <code>Union[str, Path]</code> <p>name for the output subdirectory for entailed files</p> <code>DEFAULT_ENTAILED_DIR</code> <p>Returns:</p> Type Description <code>Path</code> <p>the output path for the Turtle version of the entailed files</p> Source code in <code>ogc/na/update_vocabs.py</code> <pre><code>def make_rdf(filename: Union[str, Path], g: Graph, rootpath='/def/',\n             entailment_directory: Union[str, Path] = DEFAULT_ENTAILED_DIR,\n             provenance_metadata: ProvenanceMetadata = None,) -&gt; Path:\n    \"\"\"\n    Serializes entailed RDF graphs in several output formats for a given input\n    graph.\n\n    :param filename: the original source filename\n    :param g: [Graph][rdflib.Graph] loaded from the source file\n    :param rootpath: a path to filter concept schemes inside the Graph and infer the main one\n    :param provenance_metadata: provenance metadata (None to ignore)\n    :param entailment_directory: name for the output subdirectory for entailed files\n    :return: the output path for the Turtle version of the entailed files\n    \"\"\"\n    if not isinstance(filename, Path):\n        filename = Path(filename)\n    filename = filename.resolve()\n\n    loadable_ttl = None\n    newbasepath, canonical_filename, conceptschemeuri = \\\n        get_entailed_base_path(filename, g, rootpath, entailment_directory)\n    if newbasepath:\n        newbasepath.parent.mkdir(parents=True, exist_ok=True)\n    for entailed_format in ENTAILED_FORMATS:\n        if newbasepath:\n            newpath = newbasepath.with_suffix('.' + entailed_format['extension'])\n            if provenance_metadata:\n                provenance_metadata.generated = FileProvenanceMetadata(filename=newpath,\n                                                                       mime_type=entailed_format['mime'],\n                                                                       use_bnode=False)\n                g = generate_provenance(g + Graph(), provenance_metadata, 'ogc.na.update_vocabs')\n            g.serialize(destination=newpath, format=entailed_format['format'])\n            if entailed_format['format'] == 'ttl':\n                loadable_ttl = newpath\n\n    if filename.stem != canonical_filename:\n        logger.info(\"New file name %s -&gt; %s for %s\",\n                    filename.stem, canonical_filename, conceptschemeuri)\n\n    return loadable_ttl\n</code></pre>"},{"location":"reference/ogc/na/update_vocabs/#ogc.na.update_vocabs.setup_logging","title":"<code>setup_logging(debug=False)</code>","text":"<p>Sets up logging level and handlers (logs WARNING and ERROR to stderr).</p> <p>Parameters:</p> Name Type Description Default <code>debug</code> <code>bool</code> <p>whether to set DEBUG level</p> <code>False</code> Source code in <code>ogc/na/update_vocabs.py</code> <pre><code>def setup_logging(debug: bool = False):\n    \"\"\"\n    Sets up logging level and handlers (logs WARNING and ERROR\n    to stderr).\n\n    :param debug: whether to set DEBUG level\n    \"\"\"\n    root_logger = logging.getLogger()\n    root_logger.setLevel(logging.DEBUG if debug else logging.INFO)\n\n    fmt = logging.Formatter(fmt='%(name)s [%(levelname)s] %(message)s')\n\n    handler_out = logging.StreamHandler(sys.stdout)\n    handler_out.setLevel(logging.DEBUG)\n    handler_out.setFormatter(fmt)\n    handler_out.addFilter(lambda rec: rec.levelno &lt;= logging.INFO)\n\n    handler_err = logging.StreamHandler(sys.stderr)\n    handler_err.setLevel(logging.WARNING)\n    handler_err.setFormatter(fmt)\n\n    root_logger.addHandler(handler_out)\n    root_logger.addHandler(handler_err)\n</code></pre>"},{"location":"reference/ogc/na/util/","title":"util","text":"<p>General utilities module.</p>"},{"location":"reference/ogc/na/util/#ogc.na.util.copy_triples","title":"<code>copy_triples(src, dst=None)</code>","text":"<p>Copies all triples from one graph onto another (or a new, empty Graph if none is provided).</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>Graph</code> <p>the source Graph</p> required <code>dst</code> <code>Optional[Graph]</code> <p>the destination Graph (or <code>None</code> to create a new one)</p> <code>None</code> <p>Returns:</p> Type Description <code>Graph</code> <p>the destination Graph</p> Source code in <code>ogc/na/util.py</code> <pre><code>def copy_triples(src: Graph, dst: Optional[Graph] = None) -&gt; Graph:\n    \"\"\"\n    Copies all triples from one graph onto another (or a new, empty [Graph][rdflib.Graph]\n    if none is provided).\n\n    :param src: the source Graph\n    :param dst: the destination Graph (or `None` to create a new one)\n    :return: the destination Graph\n    \"\"\"\n    if dst is None:\n        dst = Graph()\n    for triple in src:\n        dst.add(triple)\n    return dst\n</code></pre>"},{"location":"reference/ogc/na/util/#ogc.na.util.dump_yaml","title":"<code>dump_yaml(content, filename=None, ignore_alises=True, **kwargs)</code>","text":"<p>Generates YAML output.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Any</code> <p>content to convert to YAML.</p> required <code>filename</code> <code>str | Path | None</code> <p>optional filename to dump the content into. If None, string content will be returned.</p> <code>None</code> <code>kwargs</code> <p>other args to pass to <code>yaml.dump()</code></p> <code>{}</code> Source code in <code>ogc/na/util.py</code> <pre><code>def dump_yaml(content: Any, filename: str | Path | None = None,\n              ignore_alises=True,\n              **kwargs) -&gt; str | None:\n    \"\"\"\n    Generates YAML output.\n\n    :param content: content to convert to YAML.\n    :param filename: optional filename to dump the content into. If None, string content will be returned.\n    :param kwargs: other args to pass to `yaml.dump()`\n    \"\"\"\n    kwargs.setdefault('sort_keys', False)\n    if ignore_alises:\n        class Dumper(YamlDumper):\n            def ignore_aliases(self, data) -&gt; bool:\n                return True\n    else:\n        Dumper = YamlDumper\n    if filename:\n        with open(filename, 'w') as f:\n            return yaml.dump(content, f, Dumper=Dumper, **kwargs)\n    else:\n        return yaml.dump(content, Dumper=Dumper, **kwargs)\n</code></pre>"},{"location":"reference/ogc/na/util/#ogc.na.util.entail","title":"<code>entail(g, rules, extra=None, inplace=True)</code>","text":"<p>Performs SHACL entailments on a data Graph.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>Graph</code> <p>input data Graph</p> required <code>rules</code> <code>Graph</code> <p>SHACL Graph for entailments</p> required <code>extra</code> <code>Optional[Graph]</code> <p>Graph with additional ontological information for entailment</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>if <code>True</code>, the source Graph will be modified, otherwise a new Graph will be created</p> <code>True</code> <p>Returns:</p> Type Description <code>Graph</code> <p>the resulting Graph</p> Source code in <code>ogc/na/util.py</code> <pre><code>def entail(g: Graph,\n           rules: Graph,\n           extra: Optional[Graph] = None,\n           inplace: bool = True) -&gt; Graph:\n    \"\"\"\n    Performs SHACL entailments on a data [Graph][rdflib.Graph].\n\n    :param g: input data Graph\n    :param rules: SHACL Graph for entailments\n    :param extra: Graph with additional ontological information for entailment\n    :param inplace: if `True`, the source Graph will be modified, otherwise a new\n           Graph will be created\n    :return: the resulting Graph\n    \"\"\"\n    entailed_extra = None\n    if extra:\n        entailed_extra = copy_triples(extra)\n        shacl_validate(entailed_extra, shacl_graph=rules, ont_graph=None, advanced=True, inplace=True)\n\n    if not inplace:\n        g = copy_triples(g)\n    shacl_validate(g, shacl_graph=rules, ont_graph=extra, advanced=True, inplace=True)\n\n    if entailed_extra:\n        for triple in entailed_extra:\n            g.remove(triple)\n\n    return g\n</code></pre>"},{"location":"reference/ogc/na/util/#ogc.na.util.is_url","title":"<code>is_url(url, http_only=False)</code>","text":"<p>Checks whether a string is a valid URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>the input string</p> required <code>http_only</code> <code>bool</code> <p>whether to only accept HTTP and HTTPS URLs as valid</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if this is a valid URL, otherwise <code>False</code></p> Source code in <code>ogc/na/util.py</code> <pre><code>def is_url(url: str, http_only: bool = False) -&gt; bool:\n    \"\"\"\n    Checks whether a string is a valid URL.\n\n    :param url: the input string\n    :param http_only: whether to only accept HTTP and HTTPS URLs as valid\n    :return: `True` if this is a valid URL, otherwise `False`\n    \"\"\"\n    if not url:\n        return False\n\n    parsed = urlparse(url)\n    if not parsed.scheme or not (parsed.netloc or parsed.path):\n        return False\n\n    if http_only and parsed.scheme not in ('http', 'https'):\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/ogc/na/util/#ogc.na.util.load_yaml","title":"<code>load_yaml(filename=None, content=None, url=None, safe=True)</code>","text":"<p>Loads a YAML file either from a file, a string or a URL.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str | Path | None</code> <p>YAML document file name</p> <code>None</code> <code>content</code> <code>Any | None</code> <p>str with YAML contents</p> <code>None</code> <code>url</code> <code>str | None</code> <p>url from which to retrieve the contents</p> <code>None</code> <code>safe</code> <code>bool</code> <p>whether to use safe YAMl loading</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>a dict with the loaded data</p> Source code in <code>ogc/na/util.py</code> <pre><code>def load_yaml(filename: str | Path | None = None,\n              content: Any | None = None,\n              url: str | None = None,\n              safe: bool = True) -&gt; dict:\n    \"\"\"\n    Loads a YAML file either from a file, a string or a URL.\n\n    :param filename: YAML document file name\n    :param content: str with YAML contents\n    :param url: url from which to retrieve the contents\n    :param safe: whether to use safe YAMl loading\n    :return: a dict with the loaded data\n    \"\"\"\n\n    if bool(filename) + bool(content) + bool(url) &gt; 1:\n        raise ValueError(\"One (and only one) of filename, contents and url must be provided\")\n\n    if filename:\n        with open(filename, 'r') as f:\n            return yaml.load(f, Loader=SafeYamlLoader if safe else YamlLoader)\n    else:\n        if url:\n            content = requests.get(url).text\n        return yaml.load(content, Loader=SafeYamlLoader if safe else YamlLoader)\n</code></pre>"},{"location":"reference/ogc/na/util/#ogc.na.util.parse_resources","title":"<code>parse_resources(src)</code>","text":"<p>Join one or more RDF documents or Graph's together into a new Graph.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>Union[str, Graph, list[Union[str, Graph]]]</code> <p>a path or Graph, or list thereof</p> required <p>Returns:</p> Type Description <code>Graph</code> <p>a union Graph</p> Source code in <code>ogc/na/util.py</code> <pre><code>def parse_resources(src: Union[str, Graph, list[Union[str, Graph]]]) -&gt; Graph:\n    \"\"\"\n    Join one or more RDF documents or [Graph][rdflib.Graph]'s together into\n    a new Graph.\n    :param src: a path or [Graph][rdflib.Graph], or list thereof\n    :return: a union Graph\n    \"\"\"\n    if not isinstance(src, list):\n        src = [src]\n\n    result = Graph()\n    for s in src:\n        if not isinstance(s, Graph):\n            s = Graph().parse(s)\n        copy_triples(s, result)\n\n    return result\n</code></pre>"},{"location":"reference/ogc/na/util/#ogc.na.util.validate","title":"<code>validate(g, shacl_graph, extra=None, **kwargs)</code>","text":"<p>Perform SHACL validation on a data Graph.</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <code>Graph</code> <p>input data Graph</p> required <code>shacl_graph</code> <code>Graph</code> <p>SHACL graph for validation</p> required <code>extra</code> <code>Optional[Graph]</code> <p>Graph with additional ontological information for validation</p> <code>None</code> <p>Returns:</p> Type Description <code>ValidationReport</code> <p>the resulting </p> Source code in <code>ogc/na/util.py</code> <pre><code>def validate(g: Graph, shacl_graph: Graph, extra: Optional[Graph] = None,\n             **kwargs) -&gt; ValidationReport:\n    \"\"\"\n    Perform SHACL validation on a data [Graph][rdflib.Graph].\n\n    :param g: input data Graph\n    :param shacl_graph: SHACL graph for validation\n    :param extra: Graph with additional ontological information for validation\n    :return: the resulting [][ogc.na.validation.ValidationReport]\n    \"\"\"\n    return ValidationReport(shacl_validate(data_graph=g,\n                                           shacl_graph=shacl_graph,\n                                           ont_graph=extra,\n                                           inference='rdfs',\n                                           advanced=True,\n                                           **kwargs))\n</code></pre>"},{"location":"reference/ogc/na/validation/","title":"validation","text":"<p>This module defines auxiliary classes to represent pySHACL validation reports.</p>"},{"location":"reference/ogc/na/validation/#ogc.na.validation.ProfileValidationReport","title":"<code>ProfileValidationReport</code>","text":"<p>Validation report for a given profile.</p> Source code in <code>ogc/na/validation.py</code> <pre><code>class ProfileValidationReport:\n    \"\"\"\n    Validation report for a given [profile](https://www.w3.org/TR/dx-prof/).\n    \"\"\"\n\n    def __init__(self, profile_uri: URIRef, profile_token: str, report: ValidationReport):\n        \"\"\"\n        :param profile_uri: URI for the profile\n        :param profile_token: Token for the profile\n        :param report: [ValidationReport][ogc.na.validation.ValidationReport]\n        \"\"\"\n        self.profile_uri = profile_uri\n        self.profile_token = profile_token\n        self.report = report\n\n    @property\n    def used_resources(self) -&gt; set[Union[str, Path]]:\n        return self.report.used_resources\n\n    @used_resources.setter\n    def used_resources(self, ur: set[Union[str, Path]]):\n        self.used_resources = ur\n</code></pre>"},{"location":"reference/ogc/na/validation/#ogc.na.validation.ProfileValidationReport.__init__","title":"<code>__init__(profile_uri, profile_token, report)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>profile_uri</code> <code>URIRef</code> <p>URI for the profile</p> required <code>profile_token</code> <code>str</code> <p>Token for the profile</p> required <code>report</code> <code>ValidationReport</code> <p>ValidationReport</p> required Source code in <code>ogc/na/validation.py</code> <pre><code>def __init__(self, profile_uri: URIRef, profile_token: str, report: ValidationReport):\n    \"\"\"\n    :param profile_uri: URI for the profile\n    :param profile_token: Token for the profile\n    :param report: [ValidationReport][ogc.na.validation.ValidationReport]\n    \"\"\"\n    self.profile_uri = profile_uri\n    self.profile_token = profile_token\n    self.report = report\n</code></pre>"},{"location":"reference/ogc/na/validation/#ogc.na.validation.ProfilesValidationReport","title":"<code>ProfilesValidationReport</code>","text":"<p>Class to aggregate several ProfileValidationReport's coming from different profiles.</p> <p>Results are exposed through the following fields:</p> <ul> <li><code>reports</code>: list of validation reports</li> <li><code>result</code>: <code>True</code> if all validations passed, otherwise <code>False</code></li> <li><code>graph</code>: union of all SHACL validation report Graphs</li> <li><code>text</code>: full report text coming from all validation results (separated by profile)</li> </ul> Source code in <code>ogc/na/validation.py</code> <pre><code>class ProfilesValidationReport:\n    \"\"\"\n    Class to aggregate several [ProfileValidationReport][ogc.na.validation.ProfileValidationReport]'s\n    coming from different profiles.\n\n    Results are exposed through the following fields:\n\n    * `reports`: list of [validation reports][ogc.na.validation.ProfileValidationReport]\n    * `result`: `True` if all validations passed, otherwise `False`\n    * `graph`: union of all SHACL validation report [Graph][rdflib.Graph]s\n    * `text`: full report text coming from all validation results (separated by profile)\n    \"\"\"\n\n    def __init__(self, profile_reports: list[ProfileValidationReport] = None):\n        \"\"\"\n        :param profile_reports: list of initial [validation reports][ogc.na.validation.ProfileValidationReport]\n        \"\"\"\n        self.reports: list[ProfileValidationReport] = []\n        self.result = True\n        self.graph = Graph()\n        self.text = ''\n        if profile_reports:\n            for profile_report in self.reports:\n                self.add(profile_report)\n\n    def add(self, profile_report: ProfileValidationReport):\n        \"\"\"\n        Add a new [validation report][ogc.na.validation.ProfileValidationReport].\n\n        :param profile_report:\n        \"\"\"\n        self.reports.append(profile_report)\n        self.result &amp;= profile_report.report.result\n        util.copy_triples(profile_report.report.graph, self.graph)\n        if profile_report.report.text:\n            if self.text:\n                self.text += '\\n'\n            self.text += (f\"=== {profile_report.profile_token} \"\n                          f\"({profile_report.profile_uri}) ===\\n\"\n                          f\"{profile_report.report.text}\")\n\n    def __contains__(self, item) -&gt; bool:\n        return any(r.profile_uri == item for r in self.reports)\n\n    @property\n    def used_resources(self):\n        return set(r for report in self.reports for r in report.used_resources)\n</code></pre>"},{"location":"reference/ogc/na/validation/#ogc.na.validation.ProfilesValidationReport.__init__","title":"<code>__init__(profile_reports=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>profile_reports</code> <code>list[ProfileValidationReport]</code> <p>list of initial validation reports</p> <code>None</code> Source code in <code>ogc/na/validation.py</code> <pre><code>def __init__(self, profile_reports: list[ProfileValidationReport] = None):\n    \"\"\"\n    :param profile_reports: list of initial [validation reports][ogc.na.validation.ProfileValidationReport]\n    \"\"\"\n    self.reports: list[ProfileValidationReport] = []\n    self.result = True\n    self.graph = Graph()\n    self.text = ''\n    if profile_reports:\n        for profile_report in self.reports:\n            self.add(profile_report)\n</code></pre>"},{"location":"reference/ogc/na/validation/#ogc.na.validation.ProfilesValidationReport.add","title":"<code>add(profile_report)</code>","text":"<p>Add a new validation report.</p> <p>Parameters:</p> Name Type Description Default <code>profile_report</code> <code>ProfileValidationReport</code> required Source code in <code>ogc/na/validation.py</code> <pre><code>def add(self, profile_report: ProfileValidationReport):\n    \"\"\"\n    Add a new [validation report][ogc.na.validation.ProfileValidationReport].\n\n    :param profile_report:\n    \"\"\"\n    self.reports.append(profile_report)\n    self.result &amp;= profile_report.report.result\n    util.copy_triples(profile_report.report.graph, self.graph)\n    if profile_report.report.text:\n        if self.text:\n            self.text += '\\n'\n        self.text += (f\"=== {profile_report.profile_token} \"\n                      f\"({profile_report.profile_uri}) ===\\n\"\n                      f\"{profile_report.report.text}\")\n</code></pre>"},{"location":"reference/ogc/na/input_filters/","title":"input_filters","text":""},{"location":"reference/ogc/na/input_filters/csv/","title":"csv","text":"<p>CSV Input filter for ingest_json.</p> <p>Returns CSV rows as a list. Values will always be strings (no type inference or coercion is performed).</p> <p>Configuration values:</p> <ul> <li><code>rows</code> (default: <code>dict</code>): type of elements in the result list:<ul> <li><code>dict</code>: elements will be dictionaries, with the keys taken from the <code>header-row</code>.</li> <li><code>list</code>: each resulting row will be an array values.</li> </ul> </li> <li><code>header-row</code> (default: <code>0</code>): if <code>rows</code> is <code>dict</code>, the (0-based) index of the header row. All rows before the     header row will be skipped.</li> <li><code>skip-rows</code> (default: <code>0</code>): number of rows to skip at the beginning of the file (apart from the header and pre-header     ones if <code>rows</code> is <code>dict</code>).</li> <li><code>delimiter</code> (default: <code>,</code>): field separator character</li> <li><code>quotechar</code> (default: <code>\"</code>): char used to quote (enclose) field values</li> <li><code>skip-empty-rows</code> (default: <code>True</code>): whether to omit empty rows (i.e., those with no values) from the result</li> <li><code>trim-values</code> (default: <code>False</code>): whether to apply <code>.strip()</code> to the resulting values</li> <li><code>charset</code> (default: <code>utf-8</code>): specific charset to use when opening the file</li> </ul>"},{"location":"reference/ogc/na/input_filters/xml/","title":"xml","text":"<p>XML Input filter for ingest_json.</p> <p>Processes XML files with xmltodict.</p> <p>Configuration values:</p> <ul> <li><code>process-namespaces</code> (default: <code>False</code>): Whether to process and expand namespaces (see xmltodict documentation)</li> <li><code>namespaces</code> (default: <code>None</code>): Namespace to prefix mappings dict in <code>url: prefix</code> format.</li> </ul>"}]}